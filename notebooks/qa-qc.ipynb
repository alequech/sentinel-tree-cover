{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This Jupyter notebook downloads and preprocesses Sentinel 1 and 2 tiles for large areas (at least 40 sq km). The workflow entails generating the tile coordinates, downloading the raw data, and processing (cloud and shadow removal, gap interpolation, indices, and superresolution).\n",
    "\n",
    "The notebook is broken down into the following sections:\n",
    "\n",
    "   * **Parameter definition**:\n",
    "   * **Projection functions**\n",
    "   * **Data download functions**\n",
    "   * **Cloud and shadow removal functions**\n",
    "   * **Superresoluttion functions**\n",
    "   * **Tile and folder management functions**\n",
    "   * **Function execution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are planning to download new Sentinel data, you need to have an API key to use the data provider [Sentinel Hub](https://www.sentinel-hub.com). If you do not have an API key but have access to sentinel imagery, the input data for this notebook is an entire year of:\n",
    "  * Cloud masks\n",
    "  * L1C bands 2, 8A, 11\n",
    "  * 10- and 20m L2A bands\n",
    "  * VV-VH Sentinel 1 bands\n",
    "  * Digital elevation model\n",
    "  \n",
    "  \n",
    "The data are tiled into 6300m x 6300m windows. An example of the raw data can be downloaded by running the following cell. This data can be preprocessed (cloud interpolation, super resolution, smoothing, etcetera) by running the rest of the notebook. It can then also be predicted by running `4b-predict-large-area`.\n",
    "\n",
    "## Processing units per tile\n",
    "\n",
    "There are an average of 23.5 dates per year across the training data. \n",
    "\n",
    "- Cloud probabilities: 0.2\n",
    "- Shadows: 2.4\n",
    "- S210: 50\n",
    "- S220: 18\n",
    "- S1: 12\n",
    "- DEM: 3\n",
    "- Total: 85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "import multiprocessing\n",
    "import math\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "import boto3\n",
    "from pyproj import Proj, transform\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "from scipy.ndimage import median_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../config.yaml\"):\n",
    "    with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key']\n",
    "        AWSKEY = key['awskey']\n",
    "        AWSSECRET = key['awssecret']\n",
    "else:\n",
    "    API_KEY = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/io/upload.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Constants and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Years can vary from 2017 to 2020. The value of `landscape` pulls coordinates from `../project-monitoring/database.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "\n",
    "if year > 2017:\n",
    "    dates = (f'{str(year - 1)}-11-15' , f'{str(year + 1)}-02-15')\n",
    "else: \n",
    "    dates = (f'{str(year)}-01-01' , f'{str(year + 1)}-02-15')\n",
    "    \n",
    "dates_sentinel_1 = (f'{str(year)}-01-01' , f'{str(year)}-12-31')\n",
    "SIZE = 9*5\n",
    "IMSIZE = (7*2) + (SIZE * 14)+2 # process 6320 x 6320 m blocks\n",
    "\n",
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-101.719125043164, 20.1308030360668) ../project-monitoring/qa-qc/6/2020/\n"
     ]
    }
   ],
   "source": [
    "database = pd.read_csv(\"qaqc.csv\")\n",
    "database['id'] = np.arange(0, 100)\n",
    "database.head(5)\n",
    "landscape = 6\n",
    "coords = database[database['id'] == landscape]\n",
    "coords = (float(coords['X']), float(coords['Y']))\n",
    "\n",
    "OUTPUT_FOLDER = '../project-monitoring/qa-qc/'+ str(landscape) + '/2020/'\n",
    "print(coords, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.771130</td>\n",
       "      <td>4.564620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.095755</td>\n",
       "      <td>29.364688</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-23.375391</td>\n",
       "      <td>-49.815079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.745310</td>\n",
       "      <td>-9.799948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.224530</td>\n",
       "      <td>17.194051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Y          X  VALUE  Unnamed: 3  id\n",
       "0   7.771130   4.564620    1.0         NaN   0\n",
       "1   8.095755  29.364688    1.0         NaN   1\n",
       "2 -23.375391 -49.815079    1.0         NaN   2\n",
       "3   8.745310  -9.799948    1.0         NaN   3\n",
       "4  13.224530  17.194051    1.0         NaN   4"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print(f'{f.__name__}, {np.around(te-ts, 2)}')\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Data download functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Sentinel hub, identify the following layers:\n",
    "  * CLOUD: return [CLP / 255]\n",
    "  * SHADOW: return [B02, B8A, B11]\n",
    "  * DEM: return [DEM]\n",
    "  * SENT: return [VV, VH]\n",
    "  * L2A10: return [B02,B03,B04, B08]\n",
    "  * L2A20: return [B05,B06,B07, B8A,B11,B12]\n",
    "  \n",
    "The following code block contains:\n",
    "  * `extract_dates` - return a list of calendar dates of imagery\n",
    "  * `to_int16` convert a floating point array to uint16\n",
    "  * `to_float32` convert a uint16 array ot float32\n",
    "  * `make_folder_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(date_dict: dict, year: int) -> List:\n",
    "    \"\"\" Transforms a SentinelHub date dictionary to a\n",
    "         list of integer calendar dates\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "    starting_days = np.cumsum(days_per_month)\n",
    "    for date in date_dict:\n",
    "        if date.year == year - 1:\n",
    "            dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    return dates\n",
    "\n",
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to int16, reducing storage costs by three-fold'''\n",
    "    assert np.min(array) >= 0, np.min(array)\n",
    "    assert np.max(array) <= 1, np.max(array)\n",
    "    \n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "@timing\n",
    "def to_float32(array: np.array) -> np.array:\n",
    "    \"\"\"Converts an int_x array to float32\"\"\"\n",
    "    print(f'The original max value is {np.max(array)}')\n",
    "    if not isinstance(array.flat[0], np.floating):\n",
    "        assert np.max(array) > 1\n",
    "        array = np.float32(array) / 65535.\n",
    "    assert np.max(array) <= 1\n",
    "    assert array.dtype == np.float32\n",
    "    return array\n",
    "\n",
    "def make_folder_names(step_x: int, step_y: int) -> (list, list):\n",
    "    '''Given an input tile location (step_x, step_y), identify the folder and file\n",
    "       names for each 5x5 subtile\n",
    "       \n",
    "       Parameters:\n",
    "         step_x (int):\n",
    "         step_y (int):\n",
    "\n",
    "        Returns:\n",
    "         x_vals (list)\n",
    "         y_vals (list)\n",
    "    '''\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(25):\n",
    "        y_val = (24 - i) // 5\n",
    "        x_val = 5 - ((25 - i) % 5)\n",
    "        x_val = 0 if x_val == 5 else x_val\n",
    "        x_vals.append(x_val)\n",
    "        y_vals.append(y_val)\n",
    "    y_vals = [i + (5*step_y) for i in y_vals]\n",
    "    x_vals = [i + (5*step_x) for i in x_vals]\n",
    "    return x_vals, y_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud and cloud shadow\n",
    "\n",
    "Identify clouds and cloud shadow using s2cloudless and Candra et al 2020.\n",
    "Returns cloud, shadow masks and a list of imagery dates that have <15% cloud/shadow cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def identify_clouds(bbox: List[Tuple[float, float]],\n",
    "                epsg: 'CRS', dates: dict = dates) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads and calculates cloud cover and shadow\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         dates (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         cloud_img (np.array):\n",
    "         shadows (np.array): \n",
    "         clean_steps (np.array):\n",
    "    \"\"\"\n",
    "    # Download 160 x 160 meter cloud masks, 0 - 255\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WcsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box, time=dates,\n",
    "        resx='160m',resy='160m',\n",
    "        image_format = MimeType.TIFF_d8,\n",
    "        maxcc=0.75, instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "\n",
    "    # Download 60 x 60 meter bands for shadow masking, 0 - 65535\n",
    "    shadow_request = WcsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box, time=dates,\n",
    "        resx='60m', resy='60m',\n",
    "        image_format = MimeType.TIFF_d16,\n",
    "        maxcc=0.75, instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72))\n",
    "    \n",
    "    # Convert to np.array, upscale to IMSIZE\n",
    "    cloud_img = np.array(cloud_request.get_data())\n",
    "    cloud_img = resize(cloud_img, (cloud_img.shape[0], IMSIZE, IMSIZE), order = 0,\n",
    "                       anti_aliasing = False,\n",
    "                       preserve_range = True).astype(np.uint8)\n",
    "    \n",
    "    # Identify steps with at least 15% cloud cover\n",
    "    n_cloud_px = np.sum(cloud_img > int(0.33 * 255), axis = (1, 2))\n",
    "    cloud_steps = np.argwhere(n_cloud_px > (IMSIZE**2 * 0.15))\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    \n",
    "    # Align cloud and shadow imagery dates\n",
    "    cloud_dates_dict = [x for x in cloud_request.get_dates()]\n",
    "    cloud_dates = extract_dates(cloud_dates_dict, year)\n",
    "    cloud_dates = [val for idx, val in enumerate(cloud_dates) if idx in clean_steps]\n",
    "    shadow_dates_dict = [x for x in shadow_request.get_dates()]\n",
    "    shadow_dates = extract_dates(shadow_dates_dict, year)\n",
    "    shadow_steps = [idx for idx, val in enumerate(shadow_dates) if val in cloud_dates]\n",
    "    \n",
    "    # Convert to np.array, upscale shadow to IMSIZE\n",
    "    shadow_img = np.array(shadow_request.get_data(data_filter = shadow_steps))\n",
    "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0] * (6 / 3)\n",
    "    shadow_img = shadow_img.repeat(6,axis=1).repeat(6,axis=2)\n",
    "    shadow_img = shadow_img[:, 1:-1, 1:-1, :]\n",
    "    \n",
    "    # Type assertions, size assertions\n",
    "    if not isinstance(cloud_img.flat[0], np.floating):\n",
    "        assert np.max(cloud_img) > 1\n",
    "        cloud_img = np.float32(cloud_img) / 255.\n",
    "    assert np.max(cloud_img) <= 1\n",
    "    assert cloud_img.dtype == np.float32\n",
    "    assert shadow_img.dtype == np.uint16\n",
    "    assert shadow_img.shape[0] == cloud_img.shape[0], (shadow_img.shape, cloud_img.shape)\n",
    "    \n",
    "    # Calculate shadow+cloud masks with multitemporal images (Candra et al. 2020)\n",
    "    print(f\"Shadows ({shadow_img.shape}) used {round(shadow_pus, 1)} processing units\")\n",
    "    shadows = mcm_shadow_mask(shadow_img, cloud_img)\n",
    "    \n",
    "    return cloud_img, shadows, clean_steps, np.array(cloud_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digital elevation model, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def download_dem(bbox: List[Tuple[float, float]], epsg: 'CRS') -> np.ndarray:\n",
    "    \"\"\" Downloads the DEM layer from Sentinel hub\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "    \n",
    "        Returns:\n",
    "         dem_image (arr):\n",
    "    \"\"\"\n",
    "    # Download imagery\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_size = 650\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM', bbox=box,\n",
    "                         width=dem_size, height=dem_size,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    \n",
    "    # Calculate median filter, slopde\n",
    "    dem_image = median_filter(dem_image, size = 5)\n",
    "    dem_image = calcSlope(dem_image.reshape((1, dem_size, dem_size)),\n",
    "                          np.full((dem_size, dem_size), 10), \n",
    "                          np.full((dem_size, dem_size), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((dem_size,dem_size, 1))\n",
    "    dem_image = dem_image[1:dem_size-1, 1:dem_size-1, :]\n",
    "    print(f\"DEM used {round(((IMSIZE*IMSIZE)/(512*512))*2, 1)} processing units\")\n",
    "    return dem_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sentinel 2 L2A, 10 and 20 meter bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def download_layer(bbox: List[Tuple[float, float]],\n",
    "                   clean_steps: np.ndarray, epsg: 'CRS',\n",
    "                   dates: dict = dates, year: int = year) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         clean_steps (list): list of steps to filter download request\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    \n",
    "    # Download 20 meter bands\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A20',\n",
    "            bbox=box, time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.75, resx='20m', resy='20m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "        )\n",
    "    image_dates_dict = [x for x in image_request.get_dates()]\n",
    "    image_dates = extract_dates(image_dates_dict, year)\n",
    "    steps_to_download = [i for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "    dates_to_download = [val for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "    img_20 = np.array(image_request.get_data(data_filter = steps_to_download))\n",
    "    s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "    \n",
    "    # Convert 20m bands to np.float32, ensure correct dimensions\n",
    "    if not isinstance(img_20.flat[0], np.floating):\n",
    "        print(f\"Converting S2, 20m to float32, with {np.max(img_20)} max and\"\n",
    "              f\" {len(np.unique(img_20))} unique values\")\n",
    "        assert np.max(img_20) > 1\n",
    "        img_20 = np.float32(img_20) / 65535.\n",
    "        assert np.max(img_20) <= 1\n",
    "        assert img_20.dtype == np.float32\n",
    "    \n",
    "    print(f\"Original 20 meter bands size: {img_20.shape}, using {round(s2_20_usage, 1)} PU\")\n",
    "    if img_20.shape[2]*img_20.shape[2] != 323*323:\n",
    "        print(f\"Reshaping: {img_20.shape}\")\n",
    "        img_20 = resize(img_20, (img_20.shape[0], 323, 323, img_20.shape[-1]), order = 0)\n",
    "\n",
    "    # Download 10 meter bands\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A10',\n",
    "            bbox=box, time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.75, resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "    img_10 = np.array(image_request.get_data(data_filter = steps_to_download))\n",
    "    s2_10_usage = (img_10.shape[1]*img_10.shape[2])/(512*512) * (4/3) * img_10.shape[0]\n",
    "    \n",
    "    # Convert 10 meter bands to np.float32, ensure correct dimensions\n",
    "    if not isinstance(img_10.flat[0], np.floating):\n",
    "        print(f\"Converting S2, 10m to float32, with {np.max(img_10)} max and\"\n",
    "                  f\" {s2_10_usage} PU\")\n",
    "        assert np.max(img_10) > 1\n",
    "        img_10 = np.float32(img_10) / 65535.\n",
    "        assert np.max(img_10) <= 1\n",
    "        assert img_10.dtype == np.float32\n",
    "\n",
    "    if img_10.shape[2]*img_10.shape[1] != IMSIZE*IMSIZE:\n",
    "        print(f\"Reshaping: {img_10.shape}\")\n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "    \n",
    "    # Ensure output is within correct range\n",
    "    img_10 = np.clip(img_10, 0, 1)\n",
    "    img_20 = np.clip(img_20, 0, 1)\n",
    "    return img_10, img_20, np.array(dates_to_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel 1 IW bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_dates_to_download(dates):\n",
    "    dates = np.array(dates)\n",
    "    dates_to_download = []\n",
    "    for i in starting_days:\n",
    "        s1_month = dates[dates > i]\n",
    "        s1_month = s1_month[s1_month < (i + 30)]\n",
    "        if len(s1_month) > 0:\n",
    "            dates_to_download.append(s1_month[0])\n",
    "    return dates_to_download\n",
    "\n",
    "@timing\n",
    "def download_sentinel_1(bbox: List[Tuple[float, float]],\n",
    "                        epsg: 'CRS', imsize: int = IMSIZE, \n",
    "                        dates: dict = dates_sentinel_1, layer: str = \"SENT\",\n",
    "                        year: int = year) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the GRD Sentinel 1 VV-VH layer from Sentinel Hub\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         imsize (int):\n",
    "         dates (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "         layer (str):\n",
    "         year (int): \n",
    "    \n",
    "        Returns:\n",
    "         s1 (arr):\n",
    "         image_dates (arr): \n",
    "    \"\"\"\n",
    "    # Identify the S1 orbit, imagery dates\n",
    "    source = DataSource.SENTINEL1_IW_DES if layer == \"SENT_DESC\" else DataSource.SENTINEL1_IW_ASC\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer=layer, bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            data_source=source, maxcc=1.0,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "        )\n",
    "    \n",
    "    s1_dates_dict = [x for x in image_request.get_dates()]\n",
    "    s1_dates = extract_dates(s1_dates_dict, year)\n",
    "    dates_to_download = identify_dates_to_download(s1_dates)\n",
    "    steps_to_download = [i for i, val in enumerate(s1_dates) if val in dates_to_download]\n",
    "    print(f\"The following dates will be downloaded: {dates_to_download}\")\n",
    "    data_filter = steps_to_download   \n",
    "    \n",
    "    # If the correct orbit is selected, download imagery\n",
    "    if len(image_request.download_list) > 0:\n",
    "        s1 = np.array(image_request.get_data(data_filter = data_filter))\n",
    "        print(f'The original s1 max value is {np.max(s1)}')\n",
    "        if not isinstance(s1.flat[0], np.floating):\n",
    "            assert np.max(s1) > 1\n",
    "            s1 = np.float32(s1) / 65535.\n",
    "        assert np.max(s1) <= 1\n",
    "\n",
    "        s1_usage = (2/3) * s1.shape[0] * ((s1.shape[1]*s1.shape[2]) / (512*512))\n",
    "        print(f\"Sentinel 1 used {round(s1_usage, 1)} PU for \"\n",
    "              f\" {s1.shape[0]} out of {len(image_request.download_list)} images\")\n",
    "\n",
    "        image_dates_dict = [x for x in image_request.get_dates()]\n",
    "        image_dates = extract_dates(image_dates_dict, year)\n",
    "        image_dates = [val for idx, val in enumerate(image_dates) if idx in data_filter]\n",
    "        image_dates = np.array(image_dates)\n",
    "\n",
    "        s1c = np.copy(s1)\n",
    "        s1c[np.where(s1c < 1.)] = 0\n",
    "        s1c[np.where(s1c >= 1.)] = 1.\n",
    "        n_pix_oob = np.sum(s1c, axis = (1, 2, 3))\n",
    "        to_remove = np.argwhere(n_pix_oob > (imsize*2*imsize*2)/10)\n",
    "        s1 = np.delete(s1, to_remove, 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        \n",
    "        s1_med = np.median(s1, axis = 0)\n",
    "        s1_med = np.tile(s1_med[np.newaxis, ...], (s1.shape[0], 1, 1, 1,))\n",
    "        s1[s1 == 1] = s1_med[s1 == 1]        \n",
    "        s1 = np.clip(s1, 0, 1)\n",
    "        return s1, image_dates\n",
    "    else: \n",
    "        return np.empty((0,)), np.empty((0,))\n",
    "\n",
    "\n",
    "def identify_s1_layer(coords: Tuple[float, float]) -> str:\n",
    "    \"\"\" Identifies whether to download ascending or descending \n",
    "        sentinel 1 orbit based upon predetermined geographic coverage\n",
    "        \n",
    "        Reference: https://sentinel.esa.int/web/sentinel/missions/\n",
    "                   sentinel-1/satellite-description/geographical-coverage\n",
    "        \n",
    "        Parameters:\n",
    "         coords (tuple): \n",
    "    \n",
    "        Returns:\n",
    "         layer (str): either of SENT, SENT_DESC \n",
    "    \"\"\"\n",
    "    results = rg.search(coords)\n",
    "    country = results[-1]['cc']\n",
    "    continent_name = pc.country_alpha2_to_continent_code(country)\n",
    "    if continent_name in ['AF', 'OC']:\n",
    "        layer = \"SENT\"\n",
    "    if continent_name in ['SA']:\n",
    "        if coords[0] > -7.11:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['AS']:\n",
    "        if coords[0] > 23.3:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['NA']:\n",
    "        layer = \"SENT_DESC\"\n",
    "    print(f\"The continent is: {continent_name}, and the sentinel 1 orbit is {layer}\")\n",
    "    return layer\n",
    " \n",
    "    \n",
    "def process_sentinel_1_tile(sentinel1: np.ndarray, dates: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converts a (?, X, Y, 2) Sentinel 1 array to (12, X, Y, 2)\n",
    "\n",
    "        Parameters:\n",
    "         sentinel1 (np.array):\n",
    "         dates (np.array):\n",
    "\n",
    "        Returns:\n",
    "         s1 (np.array)\n",
    "    \"\"\"\n",
    "    s1, _ = calculate_and_save_best_images(sentinel1, dates)\n",
    "    monthly = np.empty((12, sentinel1.shape[1], sentinel1.shape[2], 2))\n",
    "    index = 0\n",
    "    for start, end in zip(range(0, 72 + 6, 72 // 12), #0, 72, 6\n",
    "                          range(72 // 12, 72 + 6, 72 // 12)): # 6, 72, 6\n",
    "        monthly[index] = np.median(s1[start:end], axis = 0)\n",
    "        index += 1\n",
    "        \n",
    "    return monthly\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Superresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "MDL_PATH = \"../models/supres/\"\n",
    "\n",
    "model = tf.train.import_meta_graph(MDL_PATH + 'model.meta')\n",
    "model.restore(sess, tf.train.latest_checkpoint(MDL_PATH))\n",
    "\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"Add_6:0\")\n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "inp_bilinear = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "def superresolve(input_data, bilinear_upsample):\n",
    "    \"\"\" Worker function to run predictions on input data\n",
    "    \"\"\"\n",
    "    x = sess.run([logits], \n",
    "                 feed_dict={inp: input_data,\n",
    "                            inp_bilinear: bilinear_upsample})\n",
    "    return x[0]\n",
    "\n",
    "@timing\n",
    "def superresolve_tile(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Superresolves each 56x56 subtile in a 646x646 input tile\n",
    "       by padding the subtiles to 64x64 and removing the pad after prediction,\n",
    "       eliminating boundary artifacts\n",
    "\n",
    "        Parameters:\n",
    "         arr (arr): (?, 646, 646, 10) array\n",
    "\n",
    "        Returns:\n",
    "         superresolved (arr): (?, 646, 646, 10) array\n",
    "    \"\"\"\n",
    "    print(f\"The input array to superresolve is {arr.shape}\")\n",
    "    tiles = tile_window(646, 646, 60, 60)\n",
    "    for i in tnrange(len(tiles)):\n",
    "        subtile = tiles[i]\n",
    "        pad_l = 0 if subtile[0] >= 2 else 2\n",
    "        pad_r = 0 if subtile[0] < (644 - 60) else 2\n",
    "        pad_u = 0 if subtile[1] >= 2 else 2\n",
    "        pad_d = 0 if subtile[1] < (644 - 60) else 2\n",
    "        to_resolve = arr[:, np.max([subtile[0]-2, 0]):subtile[0]+62,\n",
    "                            np.max([subtile[1]-2, 0]):subtile[1]+62, :]\n",
    "        to_resolve = np.pad(to_resolve, ((0, 0), (pad_l, pad_r), (pad_u, pad_d), (0, 0)), 'reflect')\n",
    "        \n",
    "        bilinear = to_resolve[..., 4:]\n",
    "        \n",
    "        resolved = superresolve(\n",
    "            to_resolve, bilinear)\n",
    "        resolved = resolved[:, 2:-2, 2:-2, :]\n",
    "        arr[:, subtile[0]:subtile[0]+60, subtile[1]:subtile[1]+60, 4:] = resolved\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Tiling and folder management functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# move to src/utils/pathing.py\n",
    "def make_output_and_temp_folders(idx: str, output_folder: str = OUTPUT_FOLDER) -> None:\n",
    "    \"\"\"Makes necessary folder structures for IO of raw and processed data\n",
    "\n",
    "        Parameters:\n",
    "         idx (str)\n",
    "         output_folder (path)\n",
    "\n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "            \n",
    "    folders = ['raw/', 'raw/clouds/', 'raw/s1/', 'raw/s2_10/', 'raw/s2_20/',\n",
    "              'raw/misc/', 'processed/', 'interim']\n",
    "    \n",
    "    for folder in folders:\n",
    "        _find_and_make_dirs(output_folder + folder)\n",
    "\n",
    "\n",
    "def id_missing_px(sentinel2: np.ndarray, thresh: int = 11) -> np.ndarray:\n",
    "    \"\"\"Identifies missing (na) values in input array\n",
    "    \"\"\"\n",
    "    missing_images_0 = np.sum(sentinel2[..., :10] == 0.0, axis = (1, 2, 3))\n",
    "    missing_images_p = np.sum(sentinel2[..., :10] >= 1., axis = (1, 2, 3))\n",
    "    missing_images = missing_images_0 + missing_images_p\n",
    "    \n",
    "    missing_images = np.argwhere(missing_images >= (sentinel2.shape[1]**2) / thresh)\n",
    "    missing_images = missing_images.flatten()\n",
    "    if len(missing_images) > 0:\n",
    "        print(f\"The missing image bands (0) are: {missing_images_0}\")\n",
    "        print(f\"The missing image bands (1.0) are: {missing_images_p}\")\n",
    "    return missing_images\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download worker fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_large_tile(coord: tuple,\n",
    "                        step_x: int,\n",
    "                        step_y: int,\n",
    "                        folder: str = OUTPUT_FOLDER, \n",
    "                        year: int = year,\n",
    "                        s1_layer: str = \"SENT\") -> None:\n",
    "    \"\"\"Wrapper function to download cloud probs, Sentinel 2, Sentinel 1, and DEM\n",
    "\n",
    "        Parameters:\n",
    "         coord (tuple):\n",
    "         step_x (int):\n",
    "         step_y (int):\n",
    "         folder (path):\n",
    "         year (int):\n",
    "         s1_layer (str):\n",
    "\n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    bbx, epsg = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 80)\n",
    "    dem_bbx, _ = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 90)\n",
    "    idx = str(str(step_y) + \"_\" + str(step_x))\n",
    "    make_output_and_temp_folders(idx, folder)\n",
    "    \n",
    "    output_path = f\"{folder}output/{str(step_y*5)}/{str(step_x*5)}.npy\"\n",
    "    process_path = f\"{folder}processed/{str(step_y*5)}/{str(step_x*5)}.npy\"\n",
    "    processed = (os.path.exists(output_path) or os.path.exists(process_path))\n",
    "                 \n",
    "    clouds_file = f'{folder}raw/clouds/clouds_{idx}.hkl'\n",
    "    shadows_file = f'{folder}raw/clouds/shadows_{idx}.hkl'\n",
    "    s1_file = f'{folder}raw/s1/{idx}.hkl'\n",
    "    s1_dates_file = f'{folder}raw/misc/s1_dates_{idx}.hkl'\n",
    "    s2_10_file = f'{folder}raw/s2_10/{idx}.hkl'\n",
    "    s2_20_file = f'{folder}raw/s2_20/{idx}.hkl'\n",
    "    s2_dates_file = f'{folder}raw/misc/s2_dates_{idx}.hkl'\n",
    "    s2_file = f'{folder}raw/s2/{idx}.hkl'\n",
    "    clean_steps_file = f'{folder}raw/clouds/clean_steps_{idx}.hkl'\n",
    "\n",
    "    if not (os.path.exists(clouds_file) or processed):\n",
    "        print(f\"Downloading {clouds_file}\")\n",
    "        cloud_probs, shadows, _, image_dates = identify_clouds(bbx, epsg = epsg)\n",
    "        to_remove, _ = calculate_cloud_steps(cloud_probs, image_dates)\n",
    "\n",
    "        if len(to_remove) > 0:\n",
    "            clean_dates = np.delete(image_dates, to_remove)\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "        else:\n",
    "            clean_dates = image_dates\n",
    "\n",
    "        hkl.dump(cloud_probs, clouds_file, mode='w', compression='gzip')\n",
    "        hkl.dump(shadows, shadows_file, mode='w', compression='gzip')\n",
    "        hkl.dump(clean_dates, clean_steps_file, mode='w', compression='gzip')\n",
    "\n",
    "    if not (os.path.exists(s1_file) or processed):\n",
    "        print(f\"Downloading {s1_file}\")\n",
    "        s1_layer = identify_s1_layer((coord[1], coord[0]))\n",
    "        s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "        if s1.shape[0] == 0:\n",
    "            s1_layer = \"SENT_DESC\" if s1_layer == \"SENT\" else \"SENT\"\n",
    "            print(f'Switching to {s1_layer}')\n",
    "            s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "        s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "        hkl.dump(to_int16(s1), s1_file, mode='w', compression='gzip')\n",
    "        hkl.dump(s1_dates, s1_dates_file, mode='w', compression='gzip')\n",
    "\n",
    "    if not (os.path.exists(s2_10_file) or processed):\n",
    "        print(f\"Downloading {s2_10_file}\")\n",
    "        clean_steps = list(hkl.load(clean_steps_file))\n",
    "        cloud_probs = hkl.load(clouds_file)\n",
    "        shadows = hkl.load(shadows_file)    \n",
    "        s2_10, s2_20, s2_dates = download_layer(bbx, clean_steps = clean_steps, epsg = epsg)\n",
    "\n",
    "        # Steps to ensure that L2A, L1C derived products have exact matching dates\n",
    "        print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape},\"\n",
    "              f\" S2, {s2_10.shape}, S2d, {s2_dates.shape}\")\n",
    "        to_remove_clouds = [i for i, val in enumerate(clean_steps) if val not in s2_dates]\n",
    "        to_remove_dates = [val for i, val in enumerate(clean_steps) if val not in s2_dates]\n",
    "        if len(to_remove_clouds) >= 1:\n",
    "            print(f\"Removing {to_remove_dates} from clouds because not in S2\")\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove_clouds, 0)\n",
    "            shadows = np.delete(shadows, to_remove_clouds, 0)\n",
    "            print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape}\"\n",
    "                  f\" S2, {s2_10.shape}, S2d, {s2_dates.shape}\")\n",
    "            hkl.dump(cloud_probs, clouds_file, mode='w', compression='gzip')\n",
    "            hkl.dump(shadows, shadows_file, mode='w', compression='gzip')\n",
    "\n",
    "        assert cloud_probs.shape[0] == s2_10.shape[0], \"There is a date mismatch\"\n",
    "        hkl.dump(to_int16(s2_10), s2_10_file, mode='w', compression='gzip')\n",
    "        hkl.dump(to_int16(s2_20), s2_20_file, mode='w', compression='gzip')\n",
    "        hkl.dump(s2_dates, s2_dates_file, mode='w', compression='gzip')\n",
    "\n",
    "    if not (os.path.exists(folder + \"raw/misc/dem_{}.hkl\".format(idx)) or processed):\n",
    "        dem = download_dem(dem_bbx, epsg = epsg)\n",
    "        hkl.dump(dem, folder + \"raw/misc/dem_{}.hkl\".format(idx), mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_tile(coord: tuple,\n",
    "                       step_x: int,\n",
    "                       step_y: int,\n",
    "                       folder: str = OUTPUT_FOLDER,\n",
    "                       model: 'model' = model) -> None:\n",
    "    '''Wrapper function to interpolate clouds and temporal gaps, superresolve tiles,\n",
    "       calculate relevant indices, and save analysis-ready data to the output folder\n",
    "       \n",
    "       Parameters:\n",
    "        coord (tuple)\n",
    "        step_x (int):\n",
    "        step_y (int):\n",
    "        foldre (str):\n",
    "\n",
    "       Returns:\n",
    "        None\n",
    "    '''\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals, y_vals = make_folder_names(step_x, step_y)\n",
    "\n",
    "    processed = True\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        folder_path = f\"{str(y)}/{str(x)}\"\n",
    "        processed_exists = os.path.exists(folder + \"processed/\" + folder_path + \".hkl\")\n",
    "        output_exists = os.path.exists(folder + \"output/\" + folder_path + \".npy\")\n",
    "        if not (processed_exists or output_exists):\n",
    "            processed = False\n",
    "    if not processed:\n",
    "        print(f\"Processing because folder {folder_path}.npy does not exist\")\n",
    "\n",
    "        clouds = hkl.load(f'{folder}raw/clouds/clouds_{idx}.hkl')\n",
    "        sentinel2_10 = to_float32(hkl.load(f'{folder}raw/s2_10/{idx}.hkl'))\n",
    "        sentinel2_20 = to_float32(hkl.load(f'{folder}raw/s2_20/{idx}.hkl'))\n",
    "        dem = hkl.load(f'{folder}raw/misc/dem_{idx}.hkl')\n",
    "        image_dates = hkl.load(f'{folder}raw/misc/s2_dates_{idx}.hkl')\n",
    "        shadows = hkl.load(f'{folder}raw/clouds/shadows_{idx}.hkl')  \n",
    "        \n",
    "        sentinel2 = np.empty((sentinel2_10.shape[0], 646, 646, 10))\n",
    "        sentinel2[..., :4] = sentinel2_10\n",
    "        for band in range(6):\n",
    "            for time in range(sentinel2.shape[0]):\n",
    "                sentinel2[time, ..., band + 4] = resize(sentinel2_20[time,..., band], (646, 646), 1)\n",
    "\n",
    "        missing_px = id_missing_px(sentinel2, 3)\n",
    "        if len(missing_px) > 0:\n",
    "            print(f\"Removing {missing_px} dates due to missing data\")\n",
    "            clouds = np.delete(clouds, missing_px, axis = 0)\n",
    "            shadows = np.delete(shadows, missing_px, axis = 0)\n",
    "            image_dates = np.delete(image_dates, missing_px)\n",
    "            sentinel2 = np.delete(sentinel2, missing_px, axis = 0)\n",
    "                    \n",
    "        x, interp = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates) \n",
    "         \n",
    "        x = superresolve_tile(np.float32(x))\n",
    "        dem_i = np.tile(dem[np.newaxis, 1:-1, 1:-1, :], (x.shape[0], 1, 1, 1))\n",
    "        dem_i = dem_i / 90\n",
    "        #dem_i[dem_i > 0.25] = 0.25\n",
    "        x = np.concatenate([x, dem_i], axis = -1)\n",
    "        x = np.clip(x, 0, 1)\n",
    "        return x, image_dates, interp\n",
    "    else:\n",
    "        print(f\"Skipping because folder {folder_path}.npy exists\")\n",
    "        return None, None, None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"/\".join(OUTPUT_FOLDER.split(\"/\")[:-2]) + \"/\"\n",
    "\n",
    "def interpolate_na_vals(s2):\n",
    "    '''Interpolates NA values with closest time steps, to deal with\n",
    "       the small potential for NA values in calculating indices'''\n",
    "    for x_loc in range(s2.shape[1]):\n",
    "        for y_loc in range(s2.shape[2]):\n",
    "            n_na = np.sum(np.isnan(s2[:, x_loc, y_loc, :]), axis = 1)\n",
    "            for date in range(s2.shape[0]):\n",
    "                if n_na.flatten()[date] > 0:\n",
    "                    before, after = calculate_proximal_steps(date, np.argwhere(n_na == 0))\n",
    "                    s2[date, x_loc, y_loc, :] = ((s2[date + before, x_loc, y_loc] + \n",
    "                                                 s2[date + after, x_loc, y_loc]) / 2)\n",
    "    numb_na = np.sum(np.isnan(s2), axis = (1, 2, 3))\n",
    "    if np.sum(numb_na) > 0:\n",
    "        print(f\"There are {numb_na} NA values\")\n",
    "    return s2\n",
    "\n",
    "def process_subtiles(coord: tuple,\n",
    "                       step_x: int,\n",
    "                       step_y: int,\n",
    "                       year = 2019,\n",
    "                       path: str = INPUT_FOLDER,\n",
    "                       s2: np.ndarray = None, \n",
    "                       dates: np.ndarray = None,\n",
    "                       interp: np.ndarray = None) -> None:\n",
    "    '''Wrapper function to interpolate clouds and temporal gaps, superresolve tiles,\n",
    "       calculate relevant indices, and save analysis-ready data to the output folder\n",
    "       \n",
    "       Parameters:\n",
    "        coord (tuple)\n",
    "        step_x (int):\n",
    "        step_y (int):\n",
    "        folder (str):\n",
    "\n",
    "       Returns:\n",
    "        None\n",
    "    '''\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals, y_vals = make_folder_names(step_x, step_y)\n",
    "    s1 = hkl.load(f\"{path}/{year}/raw/s1/{idx}.hkl\")\n",
    "\n",
    "    s2 = evi(s2, verbose = True)\n",
    "    s2 = bi(s2, verbose = True)\n",
    "    s2 = msavi2(s2, verbose = True)\n",
    "    s2 = ndvi(s2, verbose = True)\n",
    "    s2 = interpolate_na_vals(s2)\n",
    "\n",
    "    index = 0\n",
    "    tiles = tile_window(IMSIZE, IMSIZE, window_size = 142)\n",
    "    for t in tiles:\n",
    "        start_x, start_y = t[0], t[1]\n",
    "        end_x = start_x + t[2]\n",
    "        end_y = start_y + t[3]\n",
    "        subset = s2[:, start_x:end_x, start_y:end_y, :]\n",
    "        interp_tile = interp[:, start_x:end_x, start_y:end_y]\n",
    "        interp_tile = np.sum(interp_tile, axis = (1, 2))\n",
    "\n",
    "        dates_tile = np.copy(dates)\n",
    "        to_remove = np.argwhere(interp_tile > ((142*142) / 10)).flatten()\n",
    "        if len(to_remove) > 0:\n",
    "            dates_tile = np.delete(dates_tile, to_remove)\n",
    "            subset = np.delete(subset, to_remove, 0)\n",
    "            print(f\"Removing {to_remove} interp, leaving {len(dates_tile)} / {len(dates)}\")\n",
    "\n",
    "        missing_px = id_missing_px(subset)\n",
    "        if len(missing_px) > 0:\n",
    "            dates_tile = np.delete(dates_tile, missing_px)\n",
    "            subset = np.delete(subset, missing_px, 0)\n",
    "            print(f\"Removing {missing_px} missing, leaving {len(dates_tile)}\")\n",
    "\n",
    "        to_remove = remove_missed_clouds(subset)\n",
    "        if len(to_remove) > 0:\n",
    "            subset = np.delete(subset, to_remove, axis = 0)\n",
    "            dates_tile = np.delete(dates_tile, to_remove)\n",
    "            print(f\"{len(to_remove)} missed clouds, leaving {len(dates_tile)}\")\n",
    "\n",
    "        subtile, _ = calculate_and_save_best_images(subset, dates_tile)\n",
    "        output = f\"{path}/{year}/processed/{y_vals[index]}/{x_vals[index]}.hkl\"\n",
    "\n",
    "        index += 1\n",
    "        \n",
    "        median = np.median(subtile, axis = 0)\n",
    "        median_s1 = np.median(s1[:, start_x:end_x, start_y:end_y, :], axis = 0)\n",
    "        median_s1 = median_s1 / 65535\n",
    "        median = np.concatenate([median, median_s1], axis = -1)\n",
    "        \n",
    "        sm = Smoother(lmbd = 800, size = subtile.shape[0], nbands = 14, dim = subtile.shape[1])\n",
    "        subtile = sm.interpolate_array(subtile)\n",
    "        subtile = np.concatenate([subtile, s1[:, start_x:end_x, start_y:end_y, :]], axis = -1)\n",
    "        subtile[..., -2:] = subtile[..., -2:] / 65535\n",
    "        \n",
    "        output_folder = \"/\".join(output.split(\"/\")[:-1])\n",
    "        if not os.path.exists(os.path.realpath(output_folder)):\n",
    "            os.makedirs(os.path.realpath(output_folder))\n",
    "        subtile = np.concatenate([subtile, median[np.newaxis]], axis = 0)\n",
    "        #The indices can range from -1 to 1, convert to 0-1 for uint16\n",
    "        subtile[..., 11:15] = np.clip(subtile[..., 11:15], -1, 1)\n",
    "        subtile[..., 11:15] = (subtile[..., 11:15] + 1) / 2\n",
    "        \n",
    "        subtile = np.clip(subtile, 0, 1)\n",
    "        subtile = to_int16(subtile)\n",
    "        print(f\"{index}: Writing {output}\")\n",
    "        assert subtile.shape[1] == 142, f\"subtile shape is {subtile.shape}\"\n",
    "        assert subtile.shape[0] == 13, f\"subtile shape is {subtile.shape}\"\n",
    "\n",
    "        hkl.dump(subtile, output, mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38.379256352338, -2.10283716627998) ../project-monitoring/qa-qc/69/2020/\n",
      "Downloading 2020 for 69\n",
      "Download 0/2500; X: 0 Y:0\n",
      "Downloading ../project-monitoring/qa-qc/69/2020/raw/clouds/clouds_0_0.hkl\n",
      "Shadows ((20, 646, 646, 6)) used 1.8 processing units\n",
      "Removing 36, time 4\n",
      "Removing 30, time 11\n",
      "Removing 36, time 13\n",
      "Removing 36, time 18\n",
      "Removing 216, time 19\n",
      "124380 1198095\n",
      "identify_clouds, 26.46\n",
      "1, Dates: [], Dist: 365, Thresh: 0.15\n",
      "2, Dates: [39], Dist: 24, Thresh: 0.01\n",
      "3, Dates: [63], Dist: 57, Thresh: 0.03\n",
      "4, Dates: [], Dist: 365, Thresh: 0.15\n",
      "5, Dates: [120], Dist: 57, Thresh: 0.03\n",
      "6, Dates: [153 163], Dist: 20, Thresh: 0.1\n",
      "7, Dates: [183 188 193], Dist: 50, Thresh: 0.03\n",
      "8, Dates: [], Dist: 365, Thresh: 0.15\n",
      "9, Dates: [243 265], Dist: 40, Thresh: 0.03\n",
      "10, Dates: [283], Dist: 85, Thresh: 0.06\n",
      "11, Dates: [], Dist: 365, Thresh: 0.15\n",
      "12, Dates: [368 398], Dist: 80, Thresh: 0.08\n",
      "Utilizing 13/20 steps\n",
      "Downloading ../project-monitoring/qa-qc/69/2020/raw/s1/0_0.hkl\n",
      "The continent is: AF, and the sentinel 1 orbit is SENT\n",
      "The following dates will be downloaded: [11, 35, 70, 94, 130, 154, 190, 214, 250, 274, 310, 346]\n",
      "The original s1 max value is 65535\n",
      "Sentinel 1 used 12.7 PU for  12 out of 30 images\n",
      "download_sentinel_1, 11.81\n",
      "Maximum time distance: 36\n",
      "Downloading ../project-monitoring/qa-qc/69/2020/raw/s2_10/0_0.hkl\n",
      "Converting S2, 20m to float32, with 65535 max and 8966 unique values\n",
      "Original 20 meter bands size: (13, 323, 323, 6), using 10.3 PU\n",
      "Converting S2, 10m to float32, with 65535 max and 27.59352620442708 PU\n",
      "download_layer, 30.5\n",
      "Shadows (13, 648, 648), clouds (13, 646, 646), S2, (13, 646, 646, 4), S2d, (13,)\n",
      "DEM used 3.2 processing units\n",
      "download_dem, 11.57\n",
      "Processing because folder 0/4.npy does not exist\n",
      "The original max value is 65535\n",
      "to_float32, 0.04\n",
      "The original max value is 65535\n",
      "to_float32, 0.01\n",
      "The missing image bands (0) are: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "The missing image bands (1.0) are: [      0       0       0       0 2120983       0  828876       0      56\n",
      "       0       8 1439592       0]\n",
      "Removing [ 4  6 11] dates due to missing data\n",
      "Interpolated 651633.0 px 0.16314317617369012%\n",
      "The input array to superresolve is (10, 646, 646, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce38e626d8c4430be4bd2d154336787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=121), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "superresolve_tile, 28.85\n",
      "Removing [3 9] interp, leaving 8 / 10\n",
      "Maximum time distance: 63\n",
      "1: Writing ../project-monitoring/qa-qc/69//2020/processed/4/0.hkl\n",
      "Removing [3 9] interp, leaving 8 / 10\n",
      "Maximum time distance: 63\n",
      "2: Writing ../project-monitoring/qa-qc/69//2020/processed/4/1.hkl\n",
      "Removing [3] interp, leaving 9 / 10\n",
      "Maximum time distance: 115\n",
      "3: Writing ../project-monitoring/qa-qc/69//2020/processed/4/2.hkl\n",
      "Removing [3] interp, leaving 9 / 10\n",
      "Maximum time distance: 115\n",
      "4: Writing ../project-monitoring/qa-qc/69//2020/processed/4/3.hkl\n",
      "Removing [3] interp, leaving 9 / 10\n",
      "Maximum time distance: 115\n",
      "5: Writing ../project-monitoring/qa-qc/69//2020/processed/4/4.hkl\n",
      "Removing [3 9] interp, leaving 8 / 10\n",
      "Maximum time distance: 63\n",
      "6: Writing ../project-monitoring/qa-qc/69//2020/processed/3/0.hkl\n",
      "Removing [3 9] interp, leaving 8 / 10\n",
      "Maximum time distance: 63\n",
      "7: Writing ../project-monitoring/qa-qc/69//2020/processed/3/1.hkl\n",
      "Removing [3 7] interp, leaving 8 / 10\n",
      "Maximum time distance: 115\n",
      "8: Writing ../project-monitoring/qa-qc/69//2020/processed/3/2.hkl\n",
      "Removing [3 6 7] interp, leaving 7 / 10\n",
      "Maximum time distance: 115\n",
      "9: Writing ../project-monitoring/qa-qc/69//2020/processed/3/3.hkl\n",
      "Removing [3 7] interp, leaving 8 / 10\n",
      "Maximum time distance: 115\n",
      "10: Writing ../project-monitoring/qa-qc/69//2020/processed/3/4.hkl\n",
      "Removing [3 7 9] interp, leaving 7 / 10\n",
      "Maximum time distance: 63\n",
      "11: Writing ../project-monitoring/qa-qc/69//2020/processed/2/0.hkl\n",
      "Removing [3 9] interp, leaving 8 / 10\n",
      "Maximum time distance: 63\n",
      "12: Writing ../project-monitoring/qa-qc/69//2020/processed/2/1.hkl\n",
      "Removing [0 3 9] interp, leaving 7 / 10\n",
      "Maximum time distance: 63\n",
      "13: Writing ../project-monitoring/qa-qc/69//2020/processed/2/2.hkl\n",
      "Removing [3 7 9] interp, leaving 7 / 10\n",
      "Maximum time distance: 63\n",
      "14: Writing ../project-monitoring/qa-qc/69//2020/processed/2/3.hkl\n",
      "Removing [0 3 9] interp, leaving 7 / 10\n",
      "Maximum time distance: 63\n",
      "15: Writing ../project-monitoring/qa-qc/69//2020/processed/2/4.hkl\n",
      "Removing [3 9] interp, leaving 8 / 10\n",
      "Maximum time distance: 63\n",
      "16: Writing ../project-monitoring/qa-qc/69//2020/processed/1/0.hkl\n",
      "Removing [3 9] interp, leaving 8 / 10\n",
      "Maximum time distance: 63\n",
      "17: Writing ../project-monitoring/qa-qc/69//2020/processed/1/1.hkl\n",
      "Removing [0 3 6 9] interp, leaving 6 / 10\n",
      "Maximum time distance: 72\n",
      "18: Writing ../project-monitoring/qa-qc/69//2020/processed/1/2.hkl\n",
      "Removing [3 7 9] interp, leaving 7 / 10\n",
      "Maximum time distance: 63\n",
      "19: Writing ../project-monitoring/qa-qc/69//2020/processed/1/3.hkl\n",
      "Removing [0 3 9] interp, leaving 7 / 10\n",
      "Maximum time distance: 63\n",
      "20: Writing ../project-monitoring/qa-qc/69//2020/processed/1/4.hkl\n",
      "Removing [3 9] interp, leaving 8 / 10\n",
      "Maximum time distance: 63\n",
      "21: Writing ../project-monitoring/qa-qc/69//2020/processed/0/0.hkl\n",
      "Removing [3 5 9] interp, leaving 7 / 10\n",
      "Maximum time distance: 63\n",
      "22: Writing ../project-monitoring/qa-qc/69//2020/processed/0/1.hkl\n",
      "Removing [3 5 9] interp, leaving 7 / 10\n",
      "Maximum time distance: 63\n",
      "23: Writing ../project-monitoring/qa-qc/69//2020/processed/0/2.hkl\n",
      "Removing [3 7 9] interp, leaving 7 / 10\n",
      "Maximum time distance: 63\n",
      "24: Writing ../project-monitoring/qa-qc/69//2020/processed/0/3.hkl\n",
      "Removing [3 9] interp, leaving 8 / 10\n",
      "Maximum time distance: 63\n",
      "25: Writing ../project-monitoring/qa-qc/69//2020/processed/0/4.hkl\n",
      "\n",
      "\n",
      "Finished in 248.1 seconds\n",
      "(34.7646971832439, -5.33993139588785) ../project-monitoring/qa-qc/70/2020/\n",
      "Downloading 2020 for 70\n",
      "Download 0/2500; X: 0 Y:0\n",
      "Downloading ../project-monitoring/qa-qc/70/2020/raw/clouds/clouds_0_0.hkl\n",
      "Shadows ((31, 646, 646, 6)) used 2.8 processing units\n",
      "Removing 36, time 2\n",
      "Removing 690, time 11\n",
      "Removing 786, time 14\n",
      "Removing 144, time 25\n",
      "Removing 144, time 29\n",
      "151759 704017\n",
      "identify_clouds, 19.76\n",
      "1, Dates: [ 2 17], Dist: 25, Thresh: 0.01\n",
      "2, Dates: [42 52], Dist: 94, Thresh: 0.06\n",
      "3, Dates: [], Dist: 365, Thresh: 0.15\n",
      "4, Dates: [], Dist: 365, Thresh: 0.15\n",
      "5, Dates: [146], Dist: 94, Thresh: 0.06\n",
      "6, Dates: [151 156 171], Dist: 20, Thresh: 0.01\n",
      "7, Dates: [181 211], Dist: 35, Thresh: 0.03\n",
      "8, Dates: [216 221 241], Dist: 20, Thresh: 0.01\n",
      "9, Dates: [246 261], Dist: 20, Thresh: 0.01\n",
      "10, Dates: [276 281 296], Dist: 30, Thresh: 0.03\n",
      "11, Dates: [311], Dist: 30, Thresh: 0.03\n",
      "12, Dates: [341 411], Dist: 10, Thresh: 0.01\n",
      "Utilizing 21/31 steps\n",
      "Downloading ../project-monitoring/qa-qc/70/2020/raw/s1/0_0.hkl\n",
      "The continent is: AF, and the sentinel 1 orbit is SENT\n",
      "The following dates will be downloaded: [9, 33, 68, 92, 128, 152, 188, 224, 248, 284, 308, 344]\n",
      "The original s1 max value is 65535\n",
      "Sentinel 1 used 12.7 PU for  12 out of 30 images\n",
      "download_sentinel_1, 9.57\n",
      "Maximum time distance: 36\n",
      "Downloading ../project-monitoring/qa-qc/70/2020/raw/s2_10/0_0.hkl\n",
      "Converting S2, 20m to float32, with 65535 max and 8940 unique values\n",
      "Original 20 meter bands size: (21, 323, 323, 6), using 16.7 PU\n",
      "Converting S2, 10m to float32, with 65535 max and 44.57415771484374 PU\n",
      "download_layer, 17.14\n",
      "Shadows (21, 648, 648), clouds (21, 646, 646), S2, (21, 646, 646, 4), S2d, (21,)\n",
      "DEM used 3.2 processing units\n",
      "download_dem, 9.36\n",
      "Processing because folder 0/4.npy does not exist\n",
      "The original max value is 65535\n",
      "to_float32, 0.11\n",
      "The original max value is 65535\n",
      "to_float32, 0.02\n",
      "Interpolated 1198278.0 px 0.14285785817291186%\n",
      "The input array to superresolve is (21, 646, 646, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1056dcc9ded640d69d20d8c5301f5368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=121), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "superresolve_tile, 56.64\n",
      "Removing [ 3 10 18] interp, leaving 18 / 21\n",
      "Maximum time distance: 104\n",
      "1: Writing ../project-monitoring/qa-qc/70//2020/processed/4/0.hkl\n",
      "Removing [ 3  8 10 18] interp, leaving 17 / 21\n",
      "Maximum time distance: 104\n",
      "2: Writing ../project-monitoring/qa-qc/70//2020/processed/4/1.hkl\n",
      "Removing [ 3  8 10 14 17 18 20] interp, leaving 14 / 21\n",
      "Missed shadow 13: 0.10102162269390993\n",
      "Maximum time distance: 104\n",
      "3: Writing ../project-monitoring/qa-qc/70//2020/processed/4/2.hkl\n",
      "Removing [ 3  9 10 13 14 17 19 20] interp, leaving 13 / 21\n",
      "Maximum time distance: 104\n",
      "4: Writing ../project-monitoring/qa-qc/70//2020/processed/4/3.hkl\n",
      "Removing [ 3  9 10 13 14] interp, leaving 16 / 21\n",
      "Maximum time distance: 104\n",
      "5: Writing ../project-monitoring/qa-qc/70//2020/processed/4/4.hkl\n",
      "Removing [ 0  1  3  9 10] interp, leaving 16 / 21\n",
      "Maximum time distance: 104\n",
      "6: Writing ../project-monitoring/qa-qc/70//2020/processed/3/0.hkl\n",
      "Removing [ 0  3  9 10] interp, leaving 17 / 21\n",
      "Maximum time distance: 104\n",
      "7: Writing ../project-monitoring/qa-qc/70//2020/processed/3/1.hkl\n",
      "Removing [ 3 10 18 20] interp, leaving 17 / 21\n",
      "Maximum time distance: 104\n",
      "8: Writing ../project-monitoring/qa-qc/70//2020/processed/3/2.hkl\n",
      "Removing [ 3  9 10 14 19] interp, leaving 16 / 21\n",
      "Maximum time distance: 104\n",
      "9: Writing ../project-monitoring/qa-qc/70//2020/processed/3/3.hkl\n",
      "Removing [ 3  9 10 17 19] interp, leaving 16 / 21\n",
      "Maximum time distance: 104\n",
      "10: Writing ../project-monitoring/qa-qc/70//2020/processed/3/4.hkl\n",
      "Removing [ 0  1  3  9 10 15 16 19] interp, leaving 13 / 21\n",
      "Maximum time distance: 104\n",
      "11: Writing ../project-monitoring/qa-qc/70//2020/processed/2/0.hkl\n",
      "Removing [ 0  3  9 10 18] interp, leaving 16 / 21\n",
      "Maximum time distance: 104\n",
      "12: Writing ../project-monitoring/qa-qc/70//2020/processed/2/1.hkl\n",
      "Removing [ 1  3  9 10 15 18 19] interp, leaving 14 / 21\n",
      "Maximum time distance: 115\n",
      "13: Writing ../project-monitoring/qa-qc/70//2020/processed/2/2.hkl\n",
      "Removing [ 3  9 10 13 19] interp, leaving 16 / 21\n",
      "Maximum time distance: 104\n",
      "14: Writing ../project-monitoring/qa-qc/70//2020/processed/2/3.hkl\n",
      "Removing [ 1  3  5  8 10 13 17] interp, leaving 14 / 21\n",
      "Maximum time distance: 104\n",
      "15: Writing ../project-monitoring/qa-qc/70//2020/processed/2/4.hkl\n",
      "Removing [ 1  3 10 17 19] interp, leaving 16 / 21\n",
      "Maximum time distance: 104\n",
      "16: Writing ../project-monitoring/qa-qc/70//2020/processed/1/0.hkl\n",
      "Removing [ 1  3  5 10 13 16 18 19] interp, leaving 13 / 21\n",
      "Maximum time distance: 115\n",
      "17: Writing ../project-monitoring/qa-qc/70//2020/processed/1/1.hkl\n",
      "Removing [ 1  3  5 10 18 19] interp, leaving 15 / 21\n",
      "Maximum time distance: 115\n",
      "18: Writing ../project-monitoring/qa-qc/70//2020/processed/1/2.hkl\n",
      "Removing [ 1  3 10 13 14 19] interp, leaving 15 / 21\n",
      "Maximum time distance: 104\n",
      "19: Writing ../project-monitoring/qa-qc/70//2020/processed/1/3.hkl\n",
      "Removing [ 3  8 10 13 14 17 18 19] interp, leaving 13 / 21\n",
      "Maximum time distance: 130\n",
      "20: Writing ../project-monitoring/qa-qc/70//2020/processed/1/4.hkl\n",
      "Removing [ 1  3 10 14 17 19] interp, leaving 15 / 21\n",
      "Maximum time distance: 104\n",
      "21: Writing ../project-monitoring/qa-qc/70//2020/processed/0/0.hkl\n",
      "Removing [ 1  3  9 10 14 19 20] interp, leaving 14 / 21\n",
      "Maximum time distance: 104\n",
      "22: Writing ../project-monitoring/qa-qc/70//2020/processed/0/1.hkl\n",
      "Removing [ 1  3  5  9 10 19] interp, leaving 15 / 21\n",
      "Maximum time distance: 104\n",
      "23: Writing ../project-monitoring/qa-qc/70//2020/processed/0/2.hkl\n",
      "Removing [ 1  3  8  9 10] interp, leaving 16 / 21\n",
      "Maximum time distance: 104\n",
      "24: Writing ../project-monitoring/qa-qc/70//2020/processed/0/3.hkl\n",
      "Removing [ 3  5  8  9 10 18] interp, leaving 15 / 21\n",
      "Maximum time distance: 104\n",
      "25: Writing ../project-monitoring/qa-qc/70//2020/processed/0/4.hkl\n",
      "\n",
      "\n",
      "Finished in 256.4 seconds\n",
      "(4.48145568169027, 10.0457535246387) ../project-monitoring/qa-qc/71/2020/\n",
      "Downloading 2020 for 71\n",
      "Download 0/2500; X: 0 Y:0\n",
      "Downloading ../project-monitoring/qa-qc/71/2020/raw/clouds/clouds_0_0.hkl\n",
      "Shadows ((41, 646, 646, 6)) used 3.6 processing units\n",
      "180 239159\n",
      "identify_clouds, 20.57\n",
      "1, Dates: [ 1 16 26], Dist: 35, Thresh: 0.03\n",
      "2, Dates: [31 36 56], Dist: 30, Thresh: 0.03\n",
      "3, Dates: [85], Dist: 29, Thresh: 0.01\n",
      "4, Dates: [110 115], Dist: 25, Thresh: 0.02\n",
      "5, Dates: [125 140], Dist: 80, Thresh: 0.06\n",
      "6, Dates: [], Dist: 365, Thresh: 0.15\n",
      "7, Dates: [], Dist: 365, Thresh: 0.15\n",
      "8, Dates: [220], Dist: 80, Thresh: 0.06\n",
      "9, Dates: [], Dist: 365, Thresh: 0.15\n",
      "10, Dates: [280 285 295], Dist: 60, Thresh: 0.06\n",
      "11, Dates: [310 315 330], Dist: 20, Thresh: 0.01\n",
      "12, Dates: [335 340 350], Dist: 5, Thresh: 0.01\n",
      "Utilizing 21/41 steps\n",
      "Downloading ../project-monitoring/qa-qc/71/2020/raw/s1/0_0.hkl\n",
      "The continent is: AF, and the sentinel 1 orbit is SENT\n",
      "The following dates will be downloaded: [2, 38, 61, 97, 121, 157, 193, 217, 253, 277, 313, 337]\n",
      "The original s1 max value is 65535\n",
      "Sentinel 1 used 12.7 PU for  12 out of 31 images\n",
      "download_sentinel_1, 8.9\n",
      "Maximum time distance: 36\n",
      "Downloading ../project-monitoring/qa-qc/71/2020/raw/s2_10/0_0.hkl\n",
      "Converting S2, 20m to float32, with 65535 max and 5756 unique values\n",
      "Original 20 meter bands size: (21, 323, 323, 6), using 16.7 PU\n",
      "Converting S2, 10m to float32, with 42172 max and 44.57415771484374 PU\n",
      "download_layer, 14.85\n",
      "Shadows (21, 648, 648), clouds (21, 646, 646), S2, (21, 646, 646, 4), S2d, (21,)\n",
      "DEM used 3.2 processing units\n",
      "download_dem, 10.16\n",
      "Processing because folder 0/4.npy does not exist\n",
      "The original max value is 42172\n",
      "to_float32, 0.12\n",
      "The original max value is 65535\n",
      "to_float32, 0.03\n",
      "Interpolated 492971.0 px 0.058771654992713314%\n",
      "The input array to superresolve is (21, 646, 646, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14de4339d6b451e9b6ec2523bc3bb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=121), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "superresolve_tile, 52.46\n",
      "Removing [6 7 8 9] interp, leaving 17 / 21\n",
      "Maximum time distance: 84\n",
      "1: Writing ../project-monitoring/qa-qc/71//2020/processed/4/0.hkl\n",
      "Removing [ 7  8 10] interp, leaving 18 / 21\n",
      "Maximum time distance: 95\n",
      "2: Writing ../project-monitoring/qa-qc/71//2020/processed/4/1.hkl\n",
      "Removing [ 7 10 11] interp, leaving 18 / 21\n",
      "Maximum time distance: 155\n",
      "3: Writing ../project-monitoring/qa-qc/71//2020/processed/4/2.hkl\n",
      "Removing [11] interp, leaving 20 / 21\n",
      "Maximum time distance: 140\n",
      "4: Writing ../project-monitoring/qa-qc/71//2020/processed/4/3.hkl\n",
      "Removing [11] interp, leaving 20 / 21\n",
      "Maximum time distance: 140\n",
      "5: Writing ../project-monitoring/qa-qc/71//2020/processed/4/4.hkl\n",
      "Removing [ 7  8  9 10 11] interp, leaving 16 / 21\n",
      "Maximum time distance: 195\n",
      "6: Writing ../project-monitoring/qa-qc/71//2020/processed/3/0.hkl\n",
      "Removing [ 7 10 11] interp, leaving 18 / 21\n",
      "Maximum time distance: 155\n",
      "7: Writing ../project-monitoring/qa-qc/71//2020/processed/3/1.hkl\n",
      "Removing [ 7  8  9 10] interp, leaving 17 / 21\n",
      "Maximum time distance: 135\n",
      "8: Writing ../project-monitoring/qa-qc/71//2020/processed/3/2.hkl\n",
      "Removing [ 7  8 10 11] interp, leaving 17 / 21\n",
      "Maximum time distance: 155\n",
      "9: Writing ../project-monitoring/qa-qc/71//2020/processed/3/3.hkl\n",
      "Removing [ 7 11] interp, leaving 19 / 21\n",
      "Maximum time distance: 140\n",
      "10: Writing ../project-monitoring/qa-qc/71//2020/processed/3/4.hkl\n",
      "Removing [ 7  9 10] interp, leaving 18 / 21\n",
      "Maximum time distance: 105\n",
      "11: Writing ../project-monitoring/qa-qc/71//2020/processed/2/0.hkl\n",
      "Removing [ 7  8  9 10 11] interp, leaving 16 / 21\n",
      "Maximum time distance: 195\n",
      "12: Writing ../project-monitoring/qa-qc/71//2020/processed/2/1.hkl\n",
      "Removing [ 7  8  9 10 11] interp, leaving 16 / 21\n",
      "Maximum time distance: 195\n",
      "13: Writing ../project-monitoring/qa-qc/71//2020/processed/2/2.hkl\n",
      "Removing [ 7  8  9 10] interp, leaving 17 / 21\n",
      "Maximum time distance: 135\n",
      "14: Writing ../project-monitoring/qa-qc/71//2020/processed/2/3.hkl\n",
      "Removing [6 7 8 9] interp, leaving 17 / 21\n",
      "Maximum time distance: 84\n",
      "15: Writing ../project-monitoring/qa-qc/71//2020/processed/2/4.hkl\n",
      "Removing [ 7 11] interp, leaving 19 / 21\n",
      "Missed shadow 10: 0.10657607617536204\n",
      "Maximum time distance: 140\n",
      "16: Writing ../project-monitoring/qa-qc/71//2020/processed/1/0.hkl\n",
      "Removing [ 7 11] interp, leaving 19 / 21\n",
      "Maximum time distance: 140\n",
      "17: Writing ../project-monitoring/qa-qc/71//2020/processed/1/1.hkl\n",
      "Removing [ 7 11] interp, leaving 19 / 21\n",
      "Maximum time distance: 140\n",
      "18: Writing ../project-monitoring/qa-qc/71//2020/processed/1/2.hkl\n",
      "Removing [ 7 11 13] interp, leaving 18 / 21\n",
      "Maximum time distance: 140\n",
      "19: Writing ../project-monitoring/qa-qc/71//2020/processed/1/3.hkl\n",
      "Removing [ 7  8  9 10 11] interp, leaving 16 / 21\n",
      "Maximum time distance: 195\n",
      "20: Writing ../project-monitoring/qa-qc/71//2020/processed/1/4.hkl\n",
      "Removing [7] interp, leaving 20 / 21\n",
      "Missed shadow 0: 0.11019638960523706\n",
      "Maximum time distance: 80\n",
      "21: Writing ../project-monitoring/qa-qc/71//2020/processed/0/0.hkl\n",
      "Removing [ 7 13] interp, leaving 19 / 21\n",
      "Missed shadow 4: 0.1571116841896449\n",
      "Missed shadow 5: 0.12785161674270978\n",
      "Maximum time distance: 80\n",
      "22: Writing ../project-monitoring/qa-qc/71//2020/processed/0/1.hkl\n",
      "Removing [7] interp, leaving 20 / 21\n",
      "Missed shadow 3: 0.1726343979369173\n",
      "Missed shadow 4: 0.13444753025193415\n",
      "Maximum time distance: 80\n",
      "23: Writing ../project-monitoring/qa-qc/71//2020/processed/0/2.hkl\n",
      "Removing [ 9 11 13] interp, leaving 18 / 21\n",
      "Maximum time distance: 140\n",
      "24: Writing ../project-monitoring/qa-qc/71//2020/processed/0/3.hkl\n",
      "Removing [ 9 10 11 13] interp, leaving 17 / 21\n",
      "Maximum time distance: 165\n",
      "25: Writing ../project-monitoring/qa-qc/71//2020/processed/0/4.hkl\n",
      "\n",
      "\n",
      "Finished in 244.3 seconds\n",
      "(2.46717103931121, 10.1270949927739) ../project-monitoring/qa-qc/72/2020/\n",
      "Downloading 2020 for 72\n",
      "Download 0/2500; X: 0 Y:0\n",
      "Downloading ../project-monitoring/qa-qc/72/2020/raw/clouds/clouds_0_0.hkl\n",
      "Shadows ((44, 646, 646, 6)) used 3.9 processing units\n",
      "52560 515354\n",
      "identify_clouds, 19.77\n",
      "1, Dates: [-1 14 24], Dist: 40, Thresh: 0.03\n",
      "2, Dates: [39 54], Dist: 24, Thresh: 0.01\n",
      "3, Dates: [63 88], Dist: 30, Thresh: 0.03\n",
      "4, Dates: [ 93 103 113], Dist: 25, Thresh: 0.01\n",
      "5, Dates: [128 133 138], Dist: 125, Thresh: 0.15\n",
      "6, Dates: [], Dist: 365, Thresh: 0.15\n",
      "7, Dates: [], Dist: 365, Thresh: 0.15\n",
      "8, Dates: [], Dist: 365, Thresh: 0.15\n",
      "9, Dates: [263], Dist: 125, Thresh: 0.15\n",
      "10, Dates: [293 303], Dist: 30, Thresh: 0.03\n",
      "11, Dates: [313 318 333], Dist: 20, Thresh: 0.01\n",
      "12, Dates: [338 348 358], Dist: 5, Thresh: 0.01\n",
      "Utilizing 22/44 steps\n",
      "Downloading ../project-monitoring/qa-qc/72/2020/raw/s1/0_0.hkl\n",
      "The continent is: AF, and the sentinel 1 orbit is SENT\n",
      "The following dates will be downloaded: [7, 43, 66, 102, 126, 162, 186, 222, 246, 282, 306, 342]\n",
      "The original s1 max value is 65535\n",
      "Sentinel 1 used 12.7 PU for  12 out of 30 images\n",
      "download_sentinel_1, 8.71\n",
      "Maximum time distance: 36\n",
      "Downloading ../project-monitoring/qa-qc/72/2020/raw/s2_10/0_0.hkl\n",
      "Converting S2, 20m to float32, with 65535 max and 7169 unique values\n",
      "Original 20 meter bands size: (22, 323, 323, 6), using 17.5 PU\n",
      "Converting S2, 10m to float32, with 50357 max and 46.69673665364583 PU\n",
      "download_layer, 16.06\n",
      "Shadows (22, 648, 648), clouds (22, 646, 646), S2, (22, 646, 646, 4), S2d, (22,)\n",
      "DEM used 3.2 processing units\n",
      "download_dem, 8.97\n",
      "Processing because folder 0/4.npy does not exist\n",
      "The original max value is 50357\n",
      "to_float32, 0.12\n",
      "The original max value is 65535\n",
      "to_float32, 0.03\n",
      "Interpolated 450602.0 px 0.05127861393133385%\n",
      "The input array to superresolve is (22, 646, 646, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ad366ae0bc482c8449b900277ddb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=121), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "superresolve_tile, 50.08\n",
      "Removing [11 13] interp, leaving 20 / 22\n",
      "Maximum time distance: 155\n",
      "1: Writing ../project-monitoring/qa-qc/72//2020/processed/4/0.hkl\n",
      "Removing [11] interp, leaving 21 / 22\n",
      "Missed shadow 3: 0.13667923031144613\n",
      "Maximum time distance: 125\n",
      "2: Writing ../project-monitoring/qa-qc/72//2020/processed/4/1.hkl\n",
      "Removing [ 5 10 11] interp, leaving 19 / 22\n",
      "Maximum time distance: 125\n",
      "3: Writing ../project-monitoring/qa-qc/72//2020/processed/4/2.hkl\n",
      "Removing [ 5 11 13] interp, leaving 19 / 22\n",
      "Missed shadow 3: 0.17074985121999603\n",
      "Maximum time distance: 155\n",
      "4: Writing ../project-monitoring/qa-qc/72//2020/processed/4/3.hkl\n",
      "Removing [ 5 11 13] interp, leaving 19 / 22\n",
      "Maximum time distance: 155\n",
      "5: Writing ../project-monitoring/qa-qc/72//2020/processed/4/4.hkl\n",
      "Removing [ 5  9 11] interp, leaving 19 / 22\n",
      "Maximum time distance: 125\n",
      "6: Writing ../project-monitoring/qa-qc/72//2020/processed/3/0.hkl\n",
      "Removing [ 4  5 11] interp, leaving 19 / 22\n",
      "Missed shadow 3: 0.19777821860741918\n",
      "Maximum time distance: 125\n",
      "7: Writing ../project-monitoring/qa-qc/72//2020/processed/3/1.hkl\n",
      "Removing [14] interp, leaving 21 / 22\n",
      "Missed shadow 4: 0.12467764332473716\n",
      "Maximum time distance: 125\n",
      "8: Writing ../project-monitoring/qa-qc/72//2020/processed/3/2.hkl\n",
      "Removing [ 5 11] interp, leaving 20 / 22\n",
      "Maximum time distance: 125\n",
      "9: Writing ../project-monitoring/qa-qc/72//2020/processed/3/3.hkl\n",
      "Removing [ 5 11 13 14] interp, leaving 18 / 22\n",
      "Maximum time distance: 165\n",
      "10: Writing ../project-monitoring/qa-qc/72//2020/processed/3/4.hkl\n",
      "Removing [ 5 11] interp, leaving 20 / 22\n",
      "Maximum time distance: 125\n",
      "11: Writing ../project-monitoring/qa-qc/72//2020/processed/2/0.hkl\n",
      "Removing [ 4  5 11] interp, leaving 19 / 22\n",
      "Maximum time distance: 125\n",
      "12: Writing ../project-monitoring/qa-qc/72//2020/processed/2/1.hkl\n",
      "Removing [11 14] interp, leaving 20 / 22\n",
      "Maximum time distance: 125\n",
      "13: Writing ../project-monitoring/qa-qc/72//2020/processed/2/2.hkl\n",
      "Removing [ 5 11 14] interp, leaving 19 / 22\n",
      "Maximum time distance: 125\n",
      "14: Writing ../project-monitoring/qa-qc/72//2020/processed/2/3.hkl\n",
      "Removing [ 5 11 14] interp, leaving 19 / 22\n",
      "Maximum time distance: 125\n",
      "15: Writing ../project-monitoring/qa-qc/72//2020/processed/2/4.hkl\n",
      "Removing [ 5 14] interp, leaving 20 / 22\n",
      "Maximum time distance: 125\n",
      "16: Writing ../project-monitoring/qa-qc/72//2020/processed/1/0.hkl\n",
      "Removing [ 4  5 11 14] interp, leaving 18 / 22\n",
      "Maximum time distance: 125\n",
      "17: Writing ../project-monitoring/qa-qc/72//2020/processed/1/1.hkl\n",
      "Removing [ 4  5 10 11 14] interp, leaving 17 / 22\n",
      "Maximum time distance: 125\n",
      "18: Writing ../project-monitoring/qa-qc/72//2020/processed/1/2.hkl\n",
      "Removing [ 5 10 11 14] interp, leaving 18 / 22\n",
      "Missed shadow 4: 0.12095814322555049\n",
      "Maximum time distance: 125\n",
      "19: Writing ../project-monitoring/qa-qc/72//2020/processed/1/3.hkl\n",
      "Removing [ 5 11 14] interp, leaving 19 / 22\n",
      "Maximum time distance: 125\n",
      "20: Writing ../project-monitoring/qa-qc/72//2020/processed/1/4.hkl\n",
      "Removing [5] interp, leaving 21 / 22\n",
      "Maximum time distance: 125\n",
      "21: Writing ../project-monitoring/qa-qc/72//2020/processed/0/0.hkl\n",
      "Removing [ 5 11 14] interp, leaving 19 / 22\n",
      "Maximum time distance: 125\n",
      "22: Writing ../project-monitoring/qa-qc/72//2020/processed/0/1.hkl\n",
      "Removing [ 5 11 14] interp, leaving 19 / 22\n",
      "Maximum time distance: 125\n",
      "23: Writing ../project-monitoring/qa-qc/72//2020/processed/0/2.hkl\n",
      "Removing [14] interp, leaving 21 / 22\n",
      "Maximum time distance: 125\n",
      "24: Writing ../project-monitoring/qa-qc/72//2020/processed/0/3.hkl\n",
      "Removing [ 5 14] interp, leaving 20 / 22\n",
      "Maximum time distance: 125\n",
      "25: Writing ../project-monitoring/qa-qc/72//2020/processed/0/4.hkl\n",
      "\n",
      "\n",
      "Finished in 242.1 seconds\n",
      "(26.601898690177304, -19.05613376362589) ../project-monitoring/qa-qc/73/2020/\n",
      "Downloading 2020 for 73\n",
      "Download 0/2500; X: 0 Y:0\n",
      "Downloading ../project-monitoring/qa-qc/73/2020/raw/clouds/clouds_0_0.hkl\n",
      "Shadows ((49, 646, 646, 6)) used 4.4 processing units\n",
      "Removing 108, time 2\n",
      "Removing 1548, time 10\n",
      "149324 77072\n",
      "identify_clouds, 22.67\n",
      "1, Dates: [-7  3 23], Dist: 50, Thresh: 0.03\n",
      "2, Dates: [53], Dist: 30, Thresh: 0.03\n",
      "3, Dates: [67], Dist: 25, Thresh: 0.01\n",
      "4, Dates: [ 92  97 117], Dist: 20, Thresh: 0.01\n",
      "5, Dates: [127 132 147], Dist: 20, Thresh: 0.01\n",
      "6, Dates: [152 162 172], Dist: 20, Thresh: 0.01\n",
      "7, Dates: [182 187 207], Dist: 20, Thresh: 0.01\n",
      "8, Dates: [212 217 237], Dist: 20, Thresh: 0.01\n",
      "9, Dates: [247 252 267], Dist: 25, Thresh: 0.01\n",
      "10, Dates: [277 282 302], Dist: 25, Thresh: 0.01\n",
      "11, Dates: [312], Dist: 45, Thresh: 0.03\n",
      "12, Dates: [357], Dist: 45, Thresh: 0.06\n",
      "Utilizing 28/49 steps\n",
      "Downloading ../project-monitoring/qa-qc/73/2020/raw/s1/0_0.hkl\n",
      "The continent is: AF, and the sentinel 1 orbit is SENT\n",
      "The following dates will be downloaded: [4, 33, 63, 92, 123, 152, 183, 219, 248, 279, 308, 339]\n",
      "The original s1 max value is 65535\n",
      "Sentinel 1 used 12.7 PU for  12 out of 61 images\n",
      "download_sentinel_1, 8.11\n",
      "Maximum time distance: 36\n",
      "Downloading ../project-monitoring/qa-qc/73/2020/raw/s2_10/0_0.hkl\n",
      "Converting S2, 20m to float32, with 53608 max and 6616 unique values\n",
      "Original 20 meter bands size: (28, 323, 323, 6), using 22.3 PU\n",
      "Converting S2, 10m to float32, with 53608 max and 59.43221028645833 PU\n",
      "download_layer, 23.21\n",
      "Shadows (28, 648, 648), clouds (28, 646, 646), S2, (28, 646, 646, 4), S2d, (28,)\n",
      "DEM used 3.2 processing units\n",
      "download_dem, 9.21\n",
      "Processing because folder 0/4.npy does not exist\n",
      "The original max value is 53608\n",
      "to_float32, 0.17\n",
      "The original max value is 53608\n",
      "to_float32, 0.03\n",
      "Interpolated 509049.0 px 0.0455163471112688%\n",
      "The input array to superresolve is (28, 646, 646, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a9baafb8d8469c9a9865adf287a9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=121), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "superresolve_tile, 71.82\n",
      "Removing [1] interp, leaving 27 / 28\n",
      "Maximum time distance: 45\n",
      "1: Writing ../project-monitoring/qa-qc/73//2020/processed/4/0.hkl\n",
      "Removing [1 3] interp, leaving 26 / 28\n",
      "Maximum time distance: 45\n",
      "2: Writing ../project-monitoring/qa-qc/73//2020/processed/4/1.hkl\n",
      "Removing [ 2 27] interp, leaving 26 / 28\n",
      "Maximum time distance: 50\n",
      "3: Writing ../project-monitoring/qa-qc/73//2020/processed/4/2.hkl\n",
      "Removing [ 2  5 12 27] interp, leaving 24 / 28\n",
      "Maximum time distance: 50\n",
      "4: Writing ../project-monitoring/qa-qc/73//2020/processed/4/3.hkl\n",
      "Removing [ 2  5 12 27] interp, leaving 24 / 28\n",
      "Maximum time distance: 50\n",
      "5: Writing ../project-monitoring/qa-qc/73//2020/processed/4/4.hkl\n",
      "Removing [ 3 12 27] interp, leaving 25 / 28\n",
      "Maximum time distance: 44\n",
      "6: Writing ../project-monitoring/qa-qc/73//2020/processed/3/0.hkl\n",
      "Removing [3] interp, leaving 27 / 28\n",
      "Maximum time distance: 45\n",
      "7: Writing ../project-monitoring/qa-qc/73//2020/processed/3/1.hkl\n",
      "Removing [ 2  3  5 12 27] interp, leaving 23 / 28\n",
      "Maximum time distance: 64\n",
      "8: Writing ../project-monitoring/qa-qc/73//2020/processed/3/2.hkl\n",
      "Removing [ 2  5  6 12 27] interp, leaving 23 / 28\n",
      "Maximum time distance: 50\n",
      "9: Writing ../project-monitoring/qa-qc/73//2020/processed/3/3.hkl\n",
      "Removing [ 2  5 12 27] interp, leaving 24 / 28\n",
      "Maximum time distance: 50\n",
      "10: Writing ../project-monitoring/qa-qc/73//2020/processed/3/4.hkl\n",
      "Removing [ 3 12 27] interp, leaving 25 / 28\n",
      "Maximum time distance: 44\n",
      "11: Writing ../project-monitoring/qa-qc/73//2020/processed/2/0.hkl\n",
      "Removing [ 3 12 27] interp, leaving 25 / 28\n",
      "Maximum time distance: 44\n",
      "12: Writing ../project-monitoring/qa-qc/73//2020/processed/2/1.hkl\n",
      "Removing [ 2  3 12] interp, leaving 25 / 28\n",
      "Maximum time distance: 64\n",
      "13: Writing ../project-monitoring/qa-qc/73//2020/processed/2/2.hkl\n",
      "Removing [ 2 12 27] interp, leaving 25 / 28\n",
      "Maximum time distance: 50\n",
      "14: Writing ../project-monitoring/qa-qc/73//2020/processed/2/3.hkl\n",
      "Removing [ 2  5 12 27] interp, leaving 24 / 28\n",
      "Maximum time distance: 50\n",
      "15: Writing ../project-monitoring/qa-qc/73//2020/processed/2/4.hkl\n",
      "Removing [27] interp, leaving 27 / 28\n",
      "Maximum time distance: 30\n",
      "16: Writing ../project-monitoring/qa-qc/73//2020/processed/1/0.hkl\n",
      "Removing [ 1 12] interp, leaving 26 / 28\n",
      "Maximum time distance: 45\n",
      "17: Writing ../project-monitoring/qa-qc/73//2020/processed/1/1.hkl\n",
      "Removing [ 1  2 12 27] interp, leaving 24 / 28\n",
      "Maximum time distance: 60\n",
      "18: Writing ../project-monitoring/qa-qc/73//2020/processed/1/2.hkl\n",
      "Removing [ 2  5 27] interp, leaving 25 / 28\n",
      "Maximum time distance: 50\n",
      "19: Writing ../project-monitoring/qa-qc/73//2020/processed/1/3.hkl\n",
      "Removing [ 2 27] interp, leaving 26 / 28\n",
      "Maximum time distance: 50\n",
      "20: Writing ../project-monitoring/qa-qc/73//2020/processed/1/4.hkl\n",
      "Removing [3] interp, leaving 27 / 28\n",
      "Maximum time distance: 45\n",
      "21: Writing ../project-monitoring/qa-qc/73//2020/processed/0/0.hkl\n",
      "Removing [12] interp, leaving 27 / 28\n",
      "Maximum time distance: 45\n",
      "22: Writing ../project-monitoring/qa-qc/73//2020/processed/0/1.hkl\n",
      "Maximum time distance: 45\n",
      "23: Writing ../project-monitoring/qa-qc/73//2020/processed/0/2.hkl\n",
      "Removing [ 1  3  5 27] interp, leaving 24 / 28\n",
      "Maximum time distance: 44\n",
      "24: Writing ../project-monitoring/qa-qc/73//2020/processed/0/3.hkl\n",
      "Removing [ 1  3  5 27] interp, leaving 24 / 28\n",
      "Maximum time distance: 44\n",
      "25: Writing ../project-monitoring/qa-qc/73//2020/processed/0/4.hkl\n",
      "\n",
      "\n",
      "Finished in 282.6 seconds\n",
      "(146.01445010306003, -20.131526683194) ../project-monitoring/qa-qc/74/2020/\n",
      "Downloading 2020 for 74\n",
      "Download 0/2500; X: 0 Y:0\n",
      "Downloading ../project-monitoring/qa-qc/74/2020/raw/clouds/clouds_0_0.hkl\n",
      "Shadows ((46, 646, 646, 6)) used 4.1 processing units\n",
      "Removing 1512, time 11\n",
      "Removing 72, time 18\n",
      "Removing 72, time 34\n",
      "Removing 972, time 37\n",
      "Removing 108, time 45\n",
      "246733 198882\n",
      "identify_clouds, 23.52\n",
      "1, Dates: [-16 -11  19], Dist: 45, Thresh: 0.03\n",
      "2, Dates: [34 44], Dist: 25, Thresh: 0.01\n",
      "3, Dates: [59 73 78], Dist: 29, Thresh: 0.03\n",
      "4, Dates: [ 98 113], Dist: 25, Thresh: 0.01\n",
      "5, Dates: [123 133 148], Dist: 20, Thresh: 0.01\n",
      "6, Dates: [153 168 178], Dist: 20, Thresh: 0.01\n",
      "7, Dates: [188 198 208], Dist: 25, Thresh: 0.01\n",
      "8, Dates: [223 228 238], Dist: 20, Thresh: 0.01\n",
      "9, Dates: [243 253 263], Dist: 20, Thresh: 0.01\n",
      "10, Dates: [273 283 303], Dist: 35, Thresh: 0.03\n",
      "11, Dates: [328 333], Dist: 25, Thresh: 0.01\n",
      "12, Dates: [338 398], Dist: 5, Thresh: 0.01\n",
      "Utilizing 32/46 steps\n",
      "Downloading ../project-monitoring/qa-qc/74/2020/raw/s1/0_0.hkl\n",
      "The continent is: OC, and the sentinel 1 orbit is SENT\n",
      "The following dates will be downloaded: []\n",
      "download_sentinel_1, 0.86\n",
      "Switching to SENT_DESC\n",
      "The following dates will be downloaded: [1, 37, 60, 96, 132, 156, 192, 216, 252, 276, 312, 336]\n",
      "The original s1 max value is 65535\n",
      "Sentinel 1 used 12.7 PU for  12 out of 31 images\n",
      "download_sentinel_1, 13.68\n",
      "Maximum time distance: 36\n",
      "Downloading ../project-monitoring/qa-qc/74/2020/raw/s2_10/0_0.hkl\n",
      "Converting S2, 20m to float32, with 62678 max and 7593 unique values\n",
      "Original 20 meter bands size: (32, 323, 323, 6), using 25.5 PU\n",
      "Converting S2, 10m to float32, with 64722 max and 67.92252604166666 PU\n",
      "download_layer, 21.98\n",
      "Shadows (32, 648, 648), clouds (32, 646, 646), S2, (32, 646, 646, 4), S2d, (32,)\n",
      "DEM used 3.2 processing units\n",
      "download_dem, 9.31\n",
      "Processing because folder 0/4.npy does not exist\n",
      "The original max value is 64722\n",
      "to_float32, 0.17\n",
      "The original max value is 62678\n",
      "to_float32, 0.04\n",
      "Interpolated 1055942.0 px 0.08261443353428938%\n",
      "The input array to superresolve is (32, 646, 646, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad91fa5407d14886823eb892e1d67666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=121), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "superresolve_tile, 76.4\n",
      "Removing [ 5  6  8 14 15 25 30] interp, leaving 25 / 32\n",
      "Maximum time distance: 65\n",
      "1: Writing ../project-monitoring/qa-qc/74//2020/processed/4/0.hkl\n",
      "Removing [ 5  6  8 14 15 23 30 31] interp, leaving 24 / 32\n",
      "Maximum time distance: 35\n",
      "2: Writing ../project-monitoring/qa-qc/74//2020/processed/4/1.hkl\n",
      "Removing [ 5  8 11 23 25 29 30] interp, leaving 25 / 32\n",
      "Maximum time distance: 70\n",
      "3: Writing ../project-monitoring/qa-qc/74//2020/processed/4/2.hkl\n",
      "Removing [ 1  5  8 14 23 25 29 30 31] interp, leaving 23 / 32\n",
      "Maximum time distance: 35\n",
      "4: Writing ../project-monitoring/qa-qc/74//2020/processed/4/3.hkl\n",
      "Removing [ 3  5  6  8 15 25 30] interp, leaving 25 / 32\n",
      "Maximum time distance: 65\n",
      "5: Writing ../project-monitoring/qa-qc/74//2020/processed/4/4.hkl\n",
      "Removing [ 5  6  8 11 15 16 25 30] interp, leaving 24 / 32\n",
      "Maximum time distance: 65\n",
      "6: Writing ../project-monitoring/qa-qc/74//2020/processed/3/0.hkl\n",
      "Removing [ 5  6  8 11 14 15 25 30] interp, leaving 24 / 32\n",
      "Maximum time distance: 65\n",
      "7: Writing ../project-monitoring/qa-qc/74//2020/processed/3/1.hkl\n",
      "Removing [ 5  6  8 11 25 29 30] interp, leaving 25 / 32\n",
      "Maximum time distance: 70\n",
      "8: Writing ../project-monitoring/qa-qc/74//2020/processed/3/2.hkl\n",
      "Removing [ 3  5  6  8 14 25] interp, leaving 26 / 32\n",
      "Maximum time distance: 60\n",
      "9: Writing ../project-monitoring/qa-qc/74//2020/processed/3/3.hkl\n",
      "Removing [ 3  5  6  8 23 31] interp, leaving 26 / 32\n",
      "Maximum time distance: 35\n",
      "10: Writing ../project-monitoring/qa-qc/74//2020/processed/3/4.hkl\n",
      "Removing [ 8 11 15 25 30] interp, leaving 27 / 32\n",
      "Maximum time distance: 65\n",
      "11: Writing ../project-monitoring/qa-qc/74//2020/processed/2/0.hkl\n",
      "Removing [ 5 15 30] interp, leaving 29 / 32\n",
      "Maximum time distance: 65\n",
      "12: Writing ../project-monitoring/qa-qc/74//2020/processed/2/1.hkl\n",
      "Removing [ 5  6  8 14 25 30 31] interp, leaving 25 / 32\n",
      "Maximum time distance: 35\n",
      "13: Writing ../project-monitoring/qa-qc/74//2020/processed/2/2.hkl\n",
      "Removing [ 5  8 14 16 25 30 31] interp, leaving 25 / 32\n",
      "Maximum time distance: 35\n",
      "14: Writing ../project-monitoring/qa-qc/74//2020/processed/2/3.hkl\n",
      "Removing [ 5  8 23 31] interp, leaving 28 / 32\n",
      "Maximum time distance: 35\n",
      "15: Writing ../project-monitoring/qa-qc/74//2020/processed/2/4.hkl\n",
      "Removing [ 8 15 30] interp, leaving 29 / 32\n",
      "Maximum time distance: 65\n",
      "16: Writing ../project-monitoring/qa-qc/74//2020/processed/1/0.hkl\n",
      "Removing [ 5  8 15 25 30] interp, leaving 27 / 32\n",
      "Maximum time distance: 65\n",
      "17: Writing ../project-monitoring/qa-qc/74//2020/processed/1/1.hkl\n",
      "Removing [ 5  6  8 15 25 30] interp, leaving 26 / 32\n",
      "Maximum time distance: 65\n",
      "18: Writing ../project-monitoring/qa-qc/74//2020/processed/1/2.hkl\n",
      "Removing [ 5  6 15 16 23 30 31] interp, leaving 25 / 32\n",
      "Maximum time distance: 34\n",
      "19: Writing ../project-monitoring/qa-qc/74//2020/processed/1/3.hkl\n",
      "Removing [ 5  8 15 23 25 30] interp, leaving 26 / 32\n",
      "Maximum time distance: 65\n",
      "20: Writing ../project-monitoring/qa-qc/74//2020/processed/1/4.hkl\n",
      "Removing [ 5  6  8 25 30] interp, leaving 27 / 32\n",
      "Maximum time distance: 65\n",
      "21: Writing ../project-monitoring/qa-qc/74//2020/processed/0/0.hkl\n",
      "Removing [ 5  6  8 15 30 31] interp, leaving 26 / 32\n",
      "Maximum time distance: 35\n",
      "22: Writing ../project-monitoring/qa-qc/74//2020/processed/0/1.hkl\n",
      "Removing [ 5  6 11 15 16 30] interp, leaving 26 / 32\n",
      "Maximum time distance: 65\n",
      "23: Writing ../project-monitoring/qa-qc/74//2020/processed/0/2.hkl\n",
      "Removing [ 5  6  8 16 23] interp, leaving 27 / 32\n",
      "Maximum time distance: 60\n",
      "24: Writing ../project-monitoring/qa-qc/74//2020/processed/0/3.hkl\n",
      "Removing [ 5  6  8 23 25 30] interp, leaving 26 / 32\n",
      "Maximum time distance: 65\n",
      "25: Writing ../project-monitoring/qa-qc/74//2020/processed/0/4.hkl\n",
      "\n",
      "\n",
      "Finished in 303.6 seconds\n"
     ]
    }
   ],
   "source": [
    "database = pd.read_csv(\"qaqc.csv\")\n",
    "database['id'] = np.arange(0, 100)\n",
    "database.head(5)\n",
    "\n",
    "for landscape in range(69, 75):\n",
    "    coords = database[database['id'] == landscape]\n",
    "    coords = (float(coords['X']), float(coords['Y']))\n",
    "\n",
    "    OUTPUT_FOLDER = '../project-monitoring/qa-qc/'+ str(landscape) + '/2020/'\n",
    "    INPUT_FOLDER = \"/\".join(OUTPUT_FOLDER.split(\"/\")[:-2]) + \"/\"\n",
    "    print(coords, OUTPUT_FOLDER)\n",
    "\n",
    "    print(f\"Downloading {year} for {landscape}\")\n",
    "\n",
    "    downloaded = 0\n",
    "    max_x = 50\n",
    "    max_y = 50\n",
    "    if not os.path.exists(os.path.realpath(OUTPUT_FOLDER)):\n",
    "                os.makedirs(os.path.realpath(OUTPUT_FOLDER))\n",
    "\n",
    "    for y_tile in range(0, 1):\n",
    "        for x_tile in range(0, 1):\n",
    "            contains = True\n",
    "            #contains = check_contains(coords, x_tile, y_tile, OUTPUT_FOLDER)\n",
    "            if contains:\n",
    "                print(f\"Download {downloaded}/{max_x*max_y}; X: {x_tile} Y:{y_tile}\")\n",
    "                downloaded += 1\n",
    "                time1 = time()\n",
    "                download_large_tile(coord = coords, step_x = x_tile, step_y = y_tile,\n",
    "                                   folder = OUTPUT_FOLDER)\n",
    "                s2, image_dates, interp = process_large_tile(coords, \n",
    "                                                             x_tile, \n",
    "                                                             y_tile,\n",
    "                                                            OUTPUT_FOLDER)\n",
    "                if s2 is not None:\n",
    "                    process_subtiles(coords, x_tile, y_tile, year = year,\n",
    "                                     path = INPUT_FOLDER,\n",
    "                                        s2 = s2, dates = image_dates, interp = interp)\n",
    "                    print(\"\\n\")\n",
    "                time2 = time()\n",
    "                print(f\"Finished in {np.around(time2 - time1, 1)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
