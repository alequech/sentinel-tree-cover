{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package import, API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "import multiprocessing\n",
    "import math\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "import boto3\n",
    "from pyproj import Proj, transform\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "from scipy.ndimage import median_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/io/upload.py\n",
    "%run ../src/tof/tof_downloading.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../config.yaml\"):\n",
    "    with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key']\n",
    "        AWSKEY = key['awskey']\n",
    "        AWSSECRET = key['awssecret']\n",
    "else:\n",
    "    API_KEY = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "\n",
    "if year > 2017:\n",
    "    dates = (f'{str(year - 1)}-11-15' , f'{str(year + 1)}-02-15')\n",
    "else: \n",
    "    dates = (f'{str(year)}-01-01' , f'{str(year + 1)}-02-15')\n",
    "    \n",
    "dates_sentinel_1 = (f'{str(year)}-01-01' , f'{str(year)}-12-31')\n",
    "SIZE = 9*5\n",
    "IMSIZE = (7*2) + (SIZE * 14)+2 # process 6320 x 6320 m blocks\n",
    "\n",
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Make the tiling ID structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_processing_area_noclip.csv\")\n",
    "\n",
    "make_tile = False\n",
    "\n",
    "if make_tile:\n",
    "    def id_tile_number(idx, col):\n",
    "        vals = data[col]\n",
    "        vals_set = sorted(np.unique(vals))\n",
    "        no = np.argwhere(vals_set == idx).flatten()\n",
    "        return str(no[0])\n",
    "\n",
    "    y_tiles = np.empty((len(data)))\n",
    "    data = data.reset_index()\n",
    "    for idx in tnrange(len(data)):\n",
    "        y_tiles[idx] = (id_tile_number(data['Y'][idx], 'Y'))\n",
    "\n",
    "    y_tiles = list(y_tiles)\n",
    "    y_tiles = [str(x) for x in y_tiles]\n",
    "    data['Y_tile'] = y_tiles\n",
    "    data.to_csv(\"final_processing_area_noclip.csv\")\n",
    "\n",
    "    x_tiles = np.empty((len(data)))\n",
    "    data = data.reset_index()\n",
    "    for idx in tnrange(len(data)):\n",
    "        x_tiles[idx] = (id_tile_number(data['X'][idx], 'X'))\n",
    "\n",
    "    x_tiles = list(x_tiles)\n",
    "    x_tiles = [str(x) for x in x_tiles]\n",
    "    data['X_tile'] = x_tiles\n",
    "    data.to_csv(\"final_processing_area_noclip.csv\")\n",
    "    \n",
    "    \n",
    "tracker = pd.DataFrame({'X_tile': [], 'Y_tile': []})\n",
    "x_tiles = [x for x in os.listdir(\"../project-monitoring/tof/\") if '.DS' not in x]\n",
    "x_tiles = [x for x in x_tiles if '.csv' not in x]\n",
    "for x_tile in x_tiles:\n",
    "    y_tiles = os.listdir(\"../project-monitoring/tof/\" + x_tile)\n",
    "    y_tiles = [y for y in y_tiles if '.DS' not in y]\n",
    "    y_tiles = [y for y in y_tiles if '.tif' not in y]\n",
    "    for y_tile in y_tiles:\n",
    "        tracker = tracker.append({'X_tile': int(x_tile), 'Y_tile': int(y_tile)}, ignore_index = True)\n",
    "\n",
    "tracker = pd.merge(tracker, data)\n",
    "tracker.to_csv(\"../project-monitoring/tof/tracker.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"processing_area_1_percent.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)\n",
    "\n",
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to uint16, reducing storage costs by three-fold'''\n",
    "    assert np.min(array) >= 0, np.min(array)\n",
    "    assert np.max(array) <= 1, np.max(array)\n",
    "    \n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "def to_float32(array: np.array) -> np.array:\n",
    "    \"\"\"Converts an int_x array to float32\"\"\"\n",
    "    print(f'The original max value is {np.max(array)}')\n",
    "    if not isinstance(array.flat[0], np.floating):\n",
    "        assert np.max(array) > 1\n",
    "        array = np.float32(array) / 65535.\n",
    "    assert np.max(array) <= 1\n",
    "    assert array.dtype == np.float32\n",
    "    return array\n",
    "\n",
    "def process_sentinel_1_tile(sentinel1: np.ndarray, dates: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converts a (?, X, Y, 2) Sentinel 1 array to (12, X, Y, 2)\n",
    "\n",
    "        Parameters:\n",
    "         sentinel1 (np.array):\n",
    "         dates (np.array):\n",
    "\n",
    "        Returns:\n",
    "         s1 (np.array)\n",
    "    \"\"\"\n",
    "    s1, _ = calculate_and_save_best_images(sentinel1, dates)\n",
    "    monthly = np.empty((12, sentinel1.shape[1], sentinel1.shape[2], 2))\n",
    "    index = 0\n",
    "    for start, end in zip(range(0, 72 + 6, 72 // 12), #0, 72, 6\n",
    "                          range(72 // 12, 72 + 6, 72 // 12)): # 6, 72, 6\n",
    "        monthly[index] = np.median(s1[start:end], axis = 0)\n",
    "        index += 1\n",
    "        \n",
    "    return monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "MDL_PATH = \"../models/supres/\"\n",
    "\n",
    "model = tf.train.import_meta_graph(MDL_PATH + 'model.meta')\n",
    "model.restore(sess, tf.train.latest_checkpoint(MDL_PATH))\n",
    "\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"Add_6:0\")\n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "inp_bilinear = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "def superresolve(input_data, bilinear_upsample):\n",
    "    \"\"\" Worker function to run predictions on input data\n",
    "    \"\"\"\n",
    "    x = sess.run([logits], \n",
    "                 feed_dict={inp: input_data,\n",
    "                            inp_bilinear: bilinear_upsample})\n",
    "    return x[0]\n",
    "\n",
    "def superresolve_tile(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Superresolves each 56x56 subtile in a 646x646 input tile\n",
    "       by padding the subtiles to 64x64 and removing the pad after prediction,\n",
    "       eliminating boundary artifacts\n",
    "\n",
    "        Parameters:\n",
    "         arr (arr): (?, 646, 646, 10) array\n",
    "\n",
    "        Returns:\n",
    "         superresolved (arr): (?, 646, 646, 10) array\n",
    "    \"\"\"\n",
    "    print(f\"The input array to superresolve is {arr.shape}\")\n",
    "\n",
    "    to_resolve = np.pad(arr, ((0, 0), (4, 4), (4, 4), (0, 0)), 'reflect')\n",
    "\n",
    "    bilinear = to_resolve[..., 4:]\n",
    "\n",
    "    resolved = superresolve(\n",
    "        to_resolve, bilinear)\n",
    "    resolved = resolved[:, 4:-4, 4:-4, :]\n",
    "    arr[..., 4:] = resolved\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bbox2(initial_bbx, expansion = 10):\n",
    "    earth_vertical = 6356.76\n",
    "    earth_horizontal = 6378.137\n",
    "    m_vertical = 1 / ((2 * math.pi / 360) * earth_vertical) / 1000\n",
    "    m_horizontal = 1 / ((2 * math.pi / 360) * earth_horizontal) / 1000\n",
    "    \n",
    "    exp_horizontal = expansion * m_horizontal\n",
    "    exp_vertical = expansion * m_vertical\n",
    "    \n",
    "    bbx = copy.deepcopy(initial_bbx)\n",
    "    bbx[0] -= exp_horizontal\n",
    "    bbx[1] -= exp_vertical\n",
    "    bbx[2] += exp_horizontal\n",
    "    bbx[3] += exp_vertical\n",
    "    return bbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import copy\n",
    "\n",
    "def write_tif(file, point, arr, dtype):\n",
    "    print(point)\n",
    "    # (min_x, min_y, max_x, max_y)\n",
    "    west = point[0]\n",
    "    east = point[2]\n",
    "    north = point[3]\n",
    "    south = point[1]\n",
    "    \n",
    "\n",
    "    arr[np.array(arr < 0)] = 0.\n",
    "    if dtype == 'uint8':\n",
    "        arr = arr.astype(np.uint8)\n",
    "    if dtype == 'uint16':\n",
    "        arr = arr.astype(np.uint16)\n",
    "\n",
    "    print(west, east)\n",
    "    transform = rasterio.transform.from_bounds(west = west, south = south,\n",
    "                                               east = east, north = north,\n",
    "                                               width = arr.shape[1], \n",
    "                                               height = arr.shape[0])\n",
    "\n",
    "    print(\"Writing\", file)\n",
    "    new_dataset = rasterio.open(file, 'w', driver = 'GTiff',\n",
    "                               height = arr.shape[0], width = arr.shape[1], count = 1,\n",
    "                               dtype = dtype,\n",
    "                               crs = '+proj=longlat +datum=WGS84 +no_defs',\n",
    "                               transform=transform)\n",
    "    new_dataset.write(arr, 1)\n",
    "    new_dataset.close()\n",
    "\n",
    "def make_output_and_temp_folders(output_folder: str) -> None:\n",
    "    \"\"\"Makes necessary folder structures for IO of raw and processed data\n",
    "\n",
    "        Parameters:\n",
    "         idx (str)\n",
    "         output_folder (path)\n",
    "\n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "            \n",
    "    folders = ['raw/', 'raw/clouds/', 'raw/misc/', 'raw/s1/',\n",
    "              'raw/s2_10/', 'raw/s2_20/']\n",
    "    \n",
    "    for folder in folders:\n",
    "        _find_and_make_dirs(output_folder + folder)\n",
    "        \n",
    "def make_bbox(initial_bbx, expansion = 10):\n",
    "    \n",
    "    #multiplier = 0.002777777777777999928 # ESA LULC pixel size\n",
    "    multiplier = 1/360\n",
    "    bbx = copy.deepcopy(initial_bbx)\n",
    "    bbx[0] -= expansion * multiplier\n",
    "    bbx[1] -= expansion * multiplier\n",
    "    bbx[2] += expansion * multiplier\n",
    "    bbx[3] += expansion * multiplier\n",
    "    return bbx\n",
    "    \n",
    "\n",
    "def download_tile(x, y, data):\n",
    "    data = data[data['Y_tile'] == int(y)]\n",
    "    data = data[data['X_tile'] == int(x)]\n",
    "    print(data)\n",
    "    data = data.reset_index(drop = True)\n",
    "    x = str(int(x))\n",
    "    y = str(int(y))\n",
    "    print(x)\n",
    "    if \".0\" in x:\n",
    "        x = x[:-2]\n",
    "    if \".0\" in y:\n",
    "        y = y[:-2]\n",
    "        \n",
    "    print(x, y)\n",
    "    val = data['VALUE']\n",
    "    initial_bbx = [data['X'][0], data['Y'][0], data['X'][0], data['Y'][0]]\n",
    "\n",
    "    #bbx = make_bbox(initial_bbx)\n",
    "    bbx = make_bbox(initial_bbx, expansion = 300/30)\n",
    "    dem_bbx = make_bbox(initial_bbx, expansion = 301/30)\n",
    "    print(bbx)\n",
    "        \n",
    "    folder = f\"../project-monitoring/tof/{str(x)}/{str(y)}/\"\n",
    "    tile_idx = f'{str(x)}X{str(y)}Y'\n",
    "    \n",
    "    make_output_and_temp_folders(folder)        \n",
    "    clouds_file = f'{folder}raw/clouds/clouds_{tile_idx}.hkl'\n",
    "    shadows_file = f'{folder}raw/clouds/shadows_{tile_idx}.hkl'\n",
    "    s1_file = f'{folder}raw/s1/{tile_idx}.hkl'\n",
    "    s1_dates_file = f'{folder}raw/misc/s1_dates_{tile_idx}.hkl'\n",
    "    s2_10_file = f'{folder}raw/s2_10/{tile_idx}.hkl'\n",
    "    s2_20_file = f'{folder}raw/s2_20/{tile_idx}.hkl'\n",
    "    s2_dates_file = f'{folder}raw/misc/s2_dates_{tile_idx}.hkl'\n",
    "    s2_file = f'{folder}raw/s2/{tile_idx}.hkl'\n",
    "    clean_steps_file = f'{folder}raw/clouds/clean_steps_{tile_idx}.hkl'\n",
    "    dem_file = f'{folder}raw/misc/dem_{tile_idx}.hkl'\n",
    "    \n",
    "    \n",
    "    if not (os.path.exists(clouds_file)):# or processed):\n",
    "        print(f\"Downloading {clouds_file}\")\n",
    "\n",
    "        cloud_probs, shadows, _, image_dates, _ = identify_clouds(bbox = bbx,\n",
    "                                                               dates = dates,\n",
    "                                                              imsize = 600,\n",
    "                                                              api_key = API_KEY,\n",
    "                                                              year = 2020)\n",
    "\n",
    "        to_remove, _ = calculate_cloud_steps(cloud_probs, image_dates)\n",
    "\n",
    "        if len(to_remove) > 0:\n",
    "            clean_dates = np.delete(image_dates, to_remove)\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "        else:\n",
    "            clean_dates = image_dates\n",
    "            \n",
    "        to_remove = subset_contiguous_sunny_dates(clean_dates)\n",
    "        if len(to_remove) > 0:\n",
    "            clean_dates = np.delete(clean_dates, to_remove)\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "\n",
    "        hkl.dump(cloud_probs, clouds_file, mode='w', compression='gzip')\n",
    "        hkl.dump(shadows, shadows_file, mode='w', compression='gzip')\n",
    "        hkl.dump(clean_dates, clean_steps_file, mode='w', compression='gzip')\n",
    "            \n",
    "    \n",
    "    if not (os.path.exists(s2_10_file)):\n",
    "        print(f\"Downloading {s2_10_file}\")\n",
    "        clean_steps = list(hkl.load(clean_steps_file))\n",
    "        cloud_probs = hkl.load(clouds_file)\n",
    "        shadows = hkl.load(shadows_file)    \n",
    "        s2_10, s2_20, s2_dates = download_sentinel_2(bbx, clean_steps = clean_steps,\n",
    "                                                     api_key = API_KEY,\n",
    "                                                     dates = dates,\n",
    "                                                     year = 2020\n",
    "                                                    )\n",
    "\n",
    "        # Steps to ensure that L2A, L1C derived products have exact matching dates\n",
    "        print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape},\"\n",
    "              f\" S2, {s2_10.shape}, S2d, {s2_dates.shape}\")\n",
    "        to_remove_clouds = [i for i, val in enumerate(clean_steps) if val not in s2_dates]\n",
    "        to_remove_dates = [val for i, val in enumerate(clean_steps) if val not in s2_dates]\n",
    "        if len(to_remove_clouds) >= 1:\n",
    "            print(f\"Removing {to_remove_dates} from clouds because not in S2\")\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove_clouds, 0)\n",
    "            shadows = np.delete(shadows, to_remove_clouds, 0)\n",
    "            print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape}\"\n",
    "                  f\" S2, {s2_10.shape}, S2d, {s2_dates.shape}\")\n",
    "            hkl.dump(cloud_probs, clouds_file, mode='w', compression='gzip')\n",
    "            hkl.dump(shadows, shadows_file, mode='w', compression='gzip')\n",
    "\n",
    "        assert cloud_probs.shape[0] == s2_10.shape[0], \"There is a date mismatch\"\n",
    "        hkl.dump(to_int16(s2_10), s2_10_file, mode='w', compression='gzip')\n",
    "        hkl.dump(to_int16(s2_20), s2_20_file, mode='w', compression='gzip')\n",
    "        hkl.dump(s2_dates, s2_dates_file, mode='w', compression='gzip')\n",
    "        \n",
    "        s210_arr = to_int16(s2_10[0, ..., 0])\n",
    "        s220_arr = to_int16(s2_20[0, ..., 0])\n",
    "            \n",
    "    if not (os.path.exists(s1_file)):\n",
    "        print(f\"Downloading {s1_file}\")\n",
    "        s1_layer = identify_s1_layer((data['X'][0], data['Y'][0]))\n",
    "        s1, s1_dates = download_sentinel_1(bbx,\n",
    "                                           layer = s1_layer,\n",
    "                                           api_key = API_KEY,\n",
    "                                           year = 2020,\n",
    "                                           dates = dates_sentinel_1)\n",
    "        if s1.shape[0] == 0:\n",
    "            s1_layer = \"SENT_DESC\" if s1_layer == \"SENT\" else \"SENT\"\n",
    "            print(f'Switching to {s1_layer}')\n",
    "            s1, s1_dates = download_sentinel_1(bbx,\n",
    "                                               layer = s1_layer,\n",
    "                                               api_key = API_KEY,\n",
    "                                               year = 2020,\n",
    "                                               dates = dates_sentinel_1)\n",
    "        s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "        hkl.dump(to_int16(s1), s1_file, mode='w', compression='gzip')\n",
    "        hkl.dump(s1_dates, s1_dates_file, mode='w', compression='gzip')\n",
    "        \n",
    "        s1_arr = to_int16(s1[0, ..., 0])\n",
    "        \n",
    "    if not os.path.exists(dem_file):\n",
    "        print(f'Downloading {dem_file}')\n",
    "        dem = download_dem(dem_bbx, api_key = API_KEY)\n",
    "        hkl.dump(dem, dem_file, mode='w', compression='gzip')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_missing_px(sentinel2: np.ndarray, thresh: int = 11) -> np.ndarray:\n",
    "    \"\"\"Identifies missing (na) values in input array\n",
    "    \"\"\"\n",
    "    missing_images_0 = np.sum(sentinel2[..., :10] == 0.0, axis = (1, 2, 3))\n",
    "    missing_images_p = np.sum(sentinel2[..., :10] >= 1., axis = (1, 2, 3))\n",
    "    missing_images = missing_images_0 + missing_images_p\n",
    "    \n",
    "    missing_images = np.argwhere(missing_images >= (sentinel2.shape[1]**2) / thresh)\n",
    "    missing_images = missing_images.flatten()\n",
    "    if len(missing_images) > 0:\n",
    "        print(f\"The missing image bands (0) are: {missing_images_0}\")\n",
    "        print(f\"The missing image bands (1.0) are: {missing_images_p}\")\n",
    "    return missing_images\n",
    "\n",
    "def process_tile(x, y, data):\n",
    "    \n",
    "    x = str(int(x))\n",
    "    y = str(int(y))\n",
    "    print(x)\n",
    "    if \".0\" in x:\n",
    "        x = x[:-2]\n",
    "    if \".0\" in y:\n",
    "        y = y[:-2]\n",
    "        \n",
    "    print(x, y)\n",
    "    \n",
    "    folder = f\"../project-monitoring/tof/{str(x)}/{str(y)}/\"\n",
    "    tile_idx = f'{str(x)}X{str(y)}Y'\n",
    "    \n",
    "    clouds_file = f'{folder}raw/clouds/clouds_{tile_idx}.hkl'\n",
    "    shadows_file = f'{folder}raw/clouds/shadows_{tile_idx}.hkl'\n",
    "    s1_file = f'{folder}raw/s1/{tile_idx}.hkl'\n",
    "    s1_dates_file = f'{folder}raw/misc/s1_dates_{tile_idx}.hkl'\n",
    "    s2_10_file = f'{folder}raw/s2_10/{tile_idx}.hkl'\n",
    "    s2_20_file = f'{folder}raw/s2_20/{tile_idx}.hkl'\n",
    "    s2_dates_file = f'{folder}raw/misc/s2_dates_{tile_idx}.hkl'\n",
    "    s2_file = f'{folder}raw/s2/{tile_idx}.hkl'\n",
    "    clean_steps_file = f'{folder}raw/clouds/clean_steps_{tile_idx}.hkl'\n",
    "    dem_file = f'{folder}raw/misc/dem_{tile_idx}.hkl'\n",
    "    \n",
    "    \n",
    "    clouds = hkl.load(clouds_file)\n",
    "    shadows = hkl.load(shadows_file)\n",
    "    s1 = hkl.load(s1_file)\n",
    "    s2_10 = to_float32(hkl.load(s2_10_file))\n",
    "    s2_20 = to_float32(hkl.load(s2_20_file))\n",
    "    dem = hkl.load(dem_file)\n",
    "    image_dates = hkl.load(s2_dates_file)\n",
    "    \n",
    "    print(f'Clouds: {clouds.shape}, \\nShadows: {shadows.shape} \\n'\n",
    "          f'S1: {s1.shape} \\nS2: {s2_10.shape}, {s2_20.shape} \\nDEM: {dem.shape}')\n",
    "    \n",
    "    width = s2_10.shape[1]\n",
    "    height = s2_20.shape[2] * 2\n",
    "    \n",
    "    if clouds.shape[1] < width:\n",
    "        pad_amt =  (width - clouds.shape[1]) // 2\n",
    "        clouds = np.pad(clouds, ((0, 0), (pad_amt, pad_amt), (0,0)), 'edge')\n",
    "        print(clouds.shape)\n",
    "        \n",
    "    if shadows.shape[1] < width:\n",
    "        pad_amt =  (width - shadows.shape[1]) // 2\n",
    "        shadows = np.pad(shadows, ((0, 0), (pad_amt, pad_amt), (0,0)), 'edge')\n",
    "        print(shadows.shape)\n",
    "        \n",
    "    if dem.shape[0] < width:\n",
    "        pad_amt =  (width - dem.shape[0]) // 2\n",
    "        dem = np.pad(dem, ((pad_amt, pad_amt), (0, 0)), 'edge')\n",
    "        print(dem.shape)\n",
    "        \n",
    "    if s2_10.shape[2] < height:\n",
    "        pad_amt =  (height - s2_10.shape[2]) / 2\n",
    "        if pad_amt % 2 == 0:\n",
    "            pad_amt = int(pad_amt)\n",
    "            s2_10 = np.pad(s2_10, ((0, 0), (0, 0), (pad_amt, pad_amt), (0,0)), 'edge')\n",
    "        else:\n",
    "            s2_10 = np.pad(s2_10, ((0, 0), (0, 0), (0, int(pad_amt * 2)), (0,0)), 'edge')\n",
    "    \n",
    "    if s2_10.shape[2] > height:\n",
    "        pad_amt =  abs(height - s2_10.shape[2])\n",
    "        print(pad_amt)\n",
    "        s2_10 = s2_10[:, :, :-pad_amt, :]\n",
    "        print(s2_10.shape)\n",
    "       \n",
    "    if dem.shape[1] < height:\n",
    "        pad_amt =  (height - dem.shape[1]) / 2\n",
    "        if pad_amt % 2 == 0:\n",
    "            pad_amt = int(pad_amt)\n",
    "            dem = np.pad(dem, ((0, 0), (pad_amt, pad_amt)), 'edge')\n",
    "        else:\n",
    "            dem = np.pad(dem, ( (0, 0), (0, int(pad_amt * 2))), 'edge')\n",
    "            \n",
    "    if dem.shape[1] > height:\n",
    "        pad_amt =  abs(height - dem.shape[1])\n",
    "        dem = dem[:, :-pad_amt]\n",
    "        \n",
    "        \n",
    "    print(f'Clouds: {clouds.shape}, \\nShadows: {shadows.shape} \\n'\n",
    "          f'S1: {s1.shape} \\nS2: {s2_10.shape}, {s2_20.shape} \\nDEM: {dem.shape}')\n",
    "            \n",
    "  \n",
    "    sentinel2 = np.empty((s2_10.shape[0], width, height, 10))\n",
    "    sentinel2[..., :4] = s2_10\n",
    "    for band in range(6):\n",
    "        for time in range(sentinel2.shape[0]):\n",
    "            sentinel2[time, ..., band + 4] = resize(s2_20[time,..., band], (width, height), 1)\n",
    "\n",
    "    missing_px = id_missing_px(sentinel2, 3)\n",
    "    if len(missing_px) > 0:\n",
    "        print(f\"Removing {missing_px} dates due to missing data\")\n",
    "        clouds = np.delete(clouds, missing_px, axis = 0)\n",
    "        shadows = np.delete(shadows, missing_px, axis = 0)\n",
    "        image_dates = np.delete(image_dates, missing_px)\n",
    "        sentinel2 = np.delete(sentinel2, missing_px, axis = 0)\n",
    "\n",
    "    x, interp = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates) \n",
    "\n",
    "    dem_i = np.tile(dem[np.newaxis, :, :, np.newaxis], (x.shape[0], 1, 1, 1))\n",
    "    dem_i = dem_i / 90\n",
    "    x = np.concatenate([x, dem_i], axis = -1)\n",
    "    x = np.clip(x, 0, 1)\n",
    "    return x, image_dates, interp, s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overlapping_windows(tiles):\n",
    "    tiles2 = np.copy(tiles)\n",
    "    n_x = np.sum(tiles2[:, 0] == 0)\n",
    "    n_y = np.sum(tiles2[:, 1] == 0)\n",
    "\n",
    "    tiles2[:n_x, 2] += 5\n",
    "    tiles2[-n_x:, 2] += 5\n",
    "    to_adjust = np.full((tiles.shape[0]), 10).astype(np.uint16)\n",
    "    \n",
    "    for i in range(len(to_adjust)):\n",
    "        if (i % n_y == 0) or ((i + 1) % n_y == 0):\n",
    "            to_adjust[i] -= 5\n",
    "    tiles2 = tiles2.astype(np.int64)\n",
    "    tiles2[:, 3] += to_adjust\n",
    "    tiles2[n_x:-n_x, 2] += 10\n",
    "    tiles2[n_x:, 0] -= 5\n",
    "    tiles2[:, 1] -=5\n",
    "    \n",
    "    tiles2[tiles2 < 0] = 0.\n",
    "\n",
    "    return tiles2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subtiles(folder, tiles):\n",
    "    \n",
    "    y_tiles = np.unique(tiles[:, 1])\n",
    "    x_tiles = np.unique(tiles[:, 0])\n",
    "    \n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "    \n",
    "    for y_tile in y_tiles:\n",
    "        _find_and_make_dirs(folder + str(y_tile) + '/')\n",
    "            \n",
    "def interpolate_na_vals(s2):\n",
    "    '''Interpolates NA values with closest time steps, to deal with\n",
    "       the small potential for NA values in calculating indices'''\n",
    "    for x_loc in range(s2.shape[1]):\n",
    "        for y_loc in range(s2.shape[2]):\n",
    "            n_na = np.sum(np.isnan(s2[:, x_loc, y_loc, :]), axis = 1)\n",
    "            for date in range(s2.shape[0]):\n",
    "                if n_na.flatten()[date] > 0:\n",
    "                    before, after = calculate_proximal_steps(date, np.argwhere(n_na == 0))\n",
    "                    s2[date, x_loc, y_loc, :] = ((s2[date + before, x_loc, y_loc] + \n",
    "                                                 s2[date + after, x_loc, y_loc]) / 2)\n",
    "    numb_na = np.sum(np.isnan(s2), axis = (1, 2, 3))\n",
    "    if np.sum(numb_na) > 0:\n",
    "        print(f\"There are {numb_na} NA values\")\n",
    "    return s2\n",
    "    \n",
    "\n",
    "def process_subtiles(x, y, s2: np.ndarray = None, \n",
    "                       dates: np.ndarray = None,\n",
    "                       interp: np.ndarray = None, s1 = None) -> None:\n",
    "    '''Wrapper function to interpolate clouds and temporal gaps, superresolve tiles,\n",
    "       calculate relevant indices, and save analysis-ready data to the output folder\n",
    "       \n",
    "       Parameters:\n",
    "        coord (tuple)\n",
    "        step_x (int):\n",
    "        step_y (int):\n",
    "        folder (str):\n",
    "\n",
    "       Returns:\n",
    "        None\n",
    "    '''\n",
    "    x = str(int(x))\n",
    "    y = str(int(y))\n",
    "    if \".0\" in x:\n",
    "        x = x[:-2]\n",
    "    if \".0\" in y:\n",
    "        y = y[:-2]\n",
    "        \n",
    "    print(x, y)\n",
    "    \n",
    "    s2 = interpolate_na_vals(s2)\n",
    "    \n",
    "    tiles_folder = tile_window(s1.shape[2], s1.shape[1], window_size = 140)\n",
    "    tiles_array = make_overlapping_windows(tiles_folder)\n",
    "    \n",
    "    \n",
    "    make_subtiles(f'../project-monitoring/tof/{str(x)}/{str(y)}/processed/',\n",
    "                  tiles_folder)\n",
    "    path = f'../project-monitoring/tof/{str(x)}/{str(y)}/processed/'\n",
    "    for t in range(len(tiles_folder)):\n",
    "        tile_folder = tiles_folder[t]\n",
    "        tile_array = tiles_array[t]\n",
    "        \n",
    "        start_x, start_y = tile_array[0], tile_array[1]\n",
    "        folder_x, folder_y = tile_folder[0], tile_folder[1]\n",
    "        end_x = start_x + tile_array[2]\n",
    "        end_y = start_y + tile_array[3]\n",
    "        subset = s2[:, start_x:end_x, start_y:end_y, :]\n",
    "        interp_tile = interp[:, start_x:end_x, start_y:end_y]\n",
    "        interp_tile = np.sum(interp_tile, axis = (1, 2))\n",
    "        \n",
    "        dates_tile = np.copy(dates)\n",
    "        to_remove = np.argwhere(interp_tile > ((150*150) / 6.67)).flatten()\n",
    "        if len(to_remove) > 0:\n",
    "            dates_tile = np.delete(dates_tile, to_remove)\n",
    "            subset = np.delete(subset, to_remove, 0)\n",
    "            print(f\"Removing {to_remove} interp, leaving {len(dates_tile)} / {len(dates)}\")\n",
    "\n",
    "        missing_px = id_missing_px(subset)\n",
    "        if len(missing_px) > 0:\n",
    "            dates_tile = np.delete(dates_tile, missing_px)\n",
    "            subset = np.delete(subset, missing_px, 0)\n",
    "            print(f\"Removing {missing_px} missing, leaving {len(dates_tile)}\")\n",
    "\n",
    "        to_remove = remove_missed_clouds(subset)\n",
    "        if len(to_remove) > 0:\n",
    "            subset = np.delete(subset, to_remove, axis = 0)\n",
    "            dates_tile = np.delete(dates_tile, to_remove)\n",
    "            print(f\"{len(to_remove)} missed clouds, leaving {len(dates_tile)}\")\n",
    "        try:\n",
    "            subtile, _ = calculate_and_save_best_images(subset, dates_tile)\n",
    "        except:\n",
    "            subtile = np.zeros((72, end_x-start_x, end_y - start_y, 11))\n",
    "            dates_tile = [0,]\n",
    "        output = f\"{path}{str(folder_y)}/{str(folder_x)}.hkl\"\n",
    "        s1_subtile = s1[:, start_x:end_x, start_y:end_y, :]\n",
    "        print(subtile.shape)\n",
    "        \n",
    "        if subtile.shape[2] == 145: \n",
    "            pad_u, pad_d = 0, 0\n",
    "            if start_y == 0:\n",
    "                pad_u = 5\n",
    "            else:\n",
    "                pad_d = 5\n",
    "            subtile = np.pad(subtile, ((0, 0,), (0, 0), (pad_u, pad_d), (0, 0)), 'reflect')\n",
    "            s1_subtile = np.pad(s1_subtile, ((0, 0,), (0, 0), (pad_u, pad_d), (0, 0)), 'reflect')\n",
    "        if subtile.shape[1] == 145:\n",
    "            pad_l, pad_r = 0, 0\n",
    "            if start_x == 0:\n",
    "                pad_l = 5\n",
    "            else:\n",
    "                pad_r = 5\n",
    "            subtile = np.pad(subtile, ((0, 0,), (pad_l, pad_r), (0, 0), (0, 0)), 'reflect')\n",
    "            s1_subtile = np.pad(s1_subtile, ((0, 0,), (pad_l, pad_r), (0, 0), (0, 0)), 'reflect')\n",
    "        \n",
    "        print(subtile.shape)\n",
    "        dem = subtile[..., -1]\n",
    "        sm = Smoother(lmbd = 800, size = subtile.shape[0], nbands = 10, dim = subtile.shape[1])\n",
    "        subtile = sm.interpolate_array(subtile[..., :-1])\n",
    "        subtile = superresolve_tile(subtile)\n",
    "        \n",
    "        subtile = np.concatenate([subtile, dem[:12, :, :, np.newaxis]], axis = -1)\n",
    "        subtile = np.concatenate([subtile,  s1_subtile], axis = -1)\n",
    "        subtile[..., -2:] = subtile[..., -2:] / 65535\n",
    "        print(subtile.shape)\n",
    "        \n",
    "        output_folder = \"/\".join(output.split(\"/\")[:-1])\n",
    "        if not os.path.exists(os.path.realpath(output_folder)):\n",
    "            os.makedirs(os.path.realpath(output_folder))\n",
    "            \n",
    "        \n",
    "        subtile = np.clip(subtile, 0, 1)\n",
    "        subtile = to_int16(subtile)\n",
    "        print(f\"Writing {output}\")\n",
    "        print(len(dates_tile))\n",
    "        assert subtile.shape[1] >= 145, f\"subtile shape is {subtile.shape}\"\n",
    "        assert subtile.shape[0] == 12, f\"subtile shape is {subtile.shape}\"\n",
    "        if len(dates_tile) < 5:\n",
    "            subtile = np.zeros_like(subtile)\n",
    "        hkl.dump(subtile, output, mode='w', compression='gzip')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>level_0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>index</th>\n",
       "      <th>Y</th>\n",
       "      <th>X</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Y_tile</th>\n",
       "      <th>X_tile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.472222</td>\n",
       "      <td>-105.027778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37.472222</td>\n",
       "      <td>-92.583333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37.472222</td>\n",
       "      <td>-92.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>37.472222</td>\n",
       "      <td>-89.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>37.472222</td>\n",
       "      <td>-89.861111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>362.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  level_0  Unnamed: 0.1  index          Y           X  VALUE  \\\n",
       "0           0        0             0      0  37.472222 -105.027778    1.0   \n",
       "1           1        1             1      1  37.472222  -92.583333    1.0   \n",
       "2           2        2             2      2  37.472222  -92.416667    1.0   \n",
       "3           3        3             3      3  37.472222  -89.916667    1.0   \n",
       "4           4        4             4      4  37.472222  -89.861111    1.0   \n",
       "\n",
       "   Unnamed: 3  Y_tile  X_tile  \n",
       "0         NaN  1622.0    96.0  \n",
       "1         NaN  1622.0   313.0  \n",
       "2         NaN  1622.0   316.0  \n",
       "3         NaN  1622.0   361.0  \n",
       "4         NaN  1622.0   362.0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print(f'{f.__name__}, {np.around(te-ts, 2)}')\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  level_0  Unnamed: 0.1  index          Y      X  VALUE  \\\n",
      "10567       10567    10567         10567  10567  23.083333 -82.25    1.0   \n",
      "\n",
      "       Unnamed: 3  Y_tile  X_tile  \n",
      "10567         NaN  1377.0   499.0  \n",
      "499\n",
      "499 1377\n",
      "[-82.27777777777777, 23.055555555555113, -82.22222222222223, 23.11111111111067]\n",
      "Downloading ../project-monitoring/tof/499/1377/raw/clouds/clouds_499X1377Y.hkl\n",
      "The original cloud size is (69, 624, 576)\n",
      "255\n",
      "100.98488789793137\n",
      "There are 22 clean steps\n",
      "Shadows ((22, 616, 568, 6)) used 0.9 processing units\n",
      "Removing 576, time 4\n",
      "Removing 384, time 5\n",
      "Removing 64, time 12\n",
      "Removing 2432, time 13\n",
      "Removing 2688, time 14\n",
      "Removing 960, time 15\n",
      "Removing 384, time 16\n",
      "249472 212544\n",
      "1, Dates: [19 24], Dist: 10, Thresh: 0.02\n",
      "2, Dates: [34], Dist: 29, Thresh: 0.01\n",
      "3, Dates: [63], Dist: 30, Thresh: 0.03\n",
      "4, Dates: [93 98], Dist: 85, Thresh: 0.06\n",
      "5, Dates: [], Dist: 365, Thresh: 0.2\n",
      "6, Dates: [], Dist: 365, Thresh: 0.2\n",
      "7, Dates: [183], Dist: 75, Thresh: 0.1\n",
      "8, Dates: [228], Dist: 45, Thresh: 0.08\n",
      "9, Dates: [243], Dist: 45, Thresh: 0.08\n",
      "10, Dates: [288], Dist: 45, Thresh: 0.15\n",
      "11, Dates: [333], Dist: 45, Thresh: 0.03\n",
      "12, Dates: [348 368 408], Dist: 15, Thresh: 0.01\n",
      "Utilizing 14/22 steps\n",
      "Downloading ../project-monitoring/tof/499/1377/raw/s2_10/499X1377Y.hkl\n",
      "Converting S2, 20m to float32, with 65535 max and 8596 unique values\n",
      "Original 20 meter bands size: (14, 309, 284, 6), using 9.4 PU\n",
      "Converting S2, 10m to float32, with 65535 max and 24.99560546875 PU\n",
      "Shadows (14, 616, 568), clouds (14, 616, 568), S2, (14, 618, 568, 4), S2d, (14,)\n",
      "Downloading ../project-monitoring/tof/499/1377/raw/s1/499X1377Y.hkl\n",
      "The continent is: AF, and the sentinel 1 orbit is SENT\n",
      "The following dates will be downloaded: [3, 39, 62, 98, 122, 158, 182, 218, 254, 278, 314, 338]\n",
      "The original s1 max value is 65535\n",
      "Sentinel 1 used 2.7 PU for  12 out of 31 images\n",
      "Maximum time distance: 36\n",
      "Downloading ../project-monitoring/tof/499/1377/raw/misc/dem_499X1377Y.hkl\n",
      "DEM used 2.7 processing units\n",
      "499\n",
      "499 1377\n",
      "The original max value is 65535\n",
      "The original max value is 65535\n",
      "Clouds: (14, 616, 568), \n",
      "Shadows: (14, 616, 568) \n",
      "S1: (12, 618, 568, 2) \n",
      "S2: (14, 618, 568, 4), (14, 309, 284, 6) \n",
      "DEM: (618, 568)\n",
      "(14, 618, 568)\n",
      "(14, 618, 568)\n",
      "Clouds: (14, 618, 568), \n",
      "Shadows: (14, 618, 568) \n",
      "S1: (12, 618, 568, 2) \n",
      "S2: (14, 618, 568, 4), (14, 309, 284, 6) \n",
      "DEM: (618, 568)\n",
      "Interpolated 940549.0 px 0.1681973828026644%\n",
      "499 1377\n",
      "Removing [6 9] interp, leaving 12 / 14\n",
      "Maximum time distance: 130\n",
      "(72, 145, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/0/0.hkl\n",
      "12\n",
      "Removing [ 0  3  6  7  9 10] interp, leaving 8 / 14\n",
      "Maximum time distance: 145\n",
      "(72, 145, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/107/0.hkl\n",
      "8\n",
      "Removing [ 1  6  7  9 10] interp, leaving 9 / 14\n",
      "Maximum time distance: 145\n",
      "(72, 145, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/214/0.hkl\n",
      "9\n",
      "Removing [3 6 9] interp, leaving 11 / 14\n",
      "Maximum time distance: 130\n",
      "(72, 145, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/321/0.hkl\n",
      "11\n",
      "Removing [6 7 8 9] interp, leaving 10 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 145, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/428/0.hkl\n",
      "10\n",
      "Removing [0 3 6 7 8 9] interp, leaving 8 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 150, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/0/119.hkl\n",
      "8\n",
      "Removing [3 5 6 7 8 9] interp, leaving 8 / 14\n",
      "Maximum time distance: 240\n",
      "(72, 150, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/107/119.hkl\n",
      "8\n",
      "Removing [ 0  3  6  7  8  9 10 12] interp, leaving 6 / 14\n",
      "Maximum time distance: 250\n",
      "(72, 150, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/214/119.hkl\n",
      "6\n",
      "Removing [ 0  3  6  9 10] interp, leaving 9 / 14\n",
      "Maximum time distance: 130\n",
      "(72, 150, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/321/119.hkl\n",
      "9\n",
      "Removing [3 4 6] interp, leaving 11 / 14\n",
      "Maximum time distance: 130\n",
      "(72, 150, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/428/119.hkl\n",
      "11\n",
      "Removing [0 6 7 9] interp, leaving 10 / 14\n",
      "Maximum time distance: 145\n",
      "(72, 150, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/0/238.hkl\n",
      "10\n",
      "Removing [0 3 6 7 8 9] interp, leaving 8 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 150, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/107/238.hkl\n",
      "8\n",
      "Removing [3 6 7 8 9] interp, leaving 9 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 150, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/214/238.hkl\n",
      "9\n",
      "Removing [0 3 6 8 9] interp, leaving 9 / 14\n",
      "Maximum time distance: 130\n",
      "(72, 150, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/321/238.hkl\n",
      "9\n",
      "Removing [0 4 6 7 8 9] interp, leaving 8 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 150, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/428/238.hkl\n",
      "8\n",
      "Removing [0 3 6 7 8 9] interp, leaving 8 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 150, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/0/358.hkl\n",
      "8\n",
      "Removing [0 1 6 7 8 9] interp, leaving 8 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 150, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/107/358.hkl\n",
      "8\n",
      "Removing [0 1 3 7 8 9] interp, leaving 8 / 14\n",
      "Maximum time distance: 150\n",
      "(72, 150, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/214/358.hkl\n",
      "8\n",
      "Removing [0 6 7 8 9] interp, leaving 9 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 150, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/321/358.hkl\n",
      "9\n",
      "Removing [0 3 6 7 8 9] interp, leaving 8 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 150, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/428/358.hkl\n",
      "8\n",
      "Removing [1 6 9] interp, leaving 11 / 14\n",
      "Maximum time distance: 130\n",
      "(72, 145, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/0/478.hkl\n",
      "11\n",
      "Removing [0 1 3 6 7 8 9] interp, leaving 7 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 145, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/107/478.hkl\n",
      "7\n",
      "Removing [0 1 7 8 9] interp, leaving 9 / 14\n",
      "Maximum time distance: 150\n",
      "(72, 145, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/214/478.hkl\n",
      "9\n",
      "Removing [0 1 7 8 9] interp, leaving 9 / 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum time distance: 150\n",
      "(72, 145, 150, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/321/478.hkl\n",
      "9\n",
      "Removing [1 3 6 7 8 9] interp, leaving 8 / 14\n",
      "Maximum time distance: 235\n",
      "(72, 145, 145, 11)\n",
      "(72, 150, 150, 11)\n",
      "The input array to superresolve is (12, 150, 150, 10)\n",
      "(12, 150, 150, 13)\n",
      "Writing ../project-monitoring/tof/499/1377/processed/428/478.hkl\n",
      "8\n",
      "Finished in 206.0 seconds\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "%run ../src/tof/tof_downloading.py\n",
    "from time import time\n",
    "\n",
    "time1 = time()\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "year = 2020\n",
    "dates = (f'{str(year)}-01-01' , f'{str(year + 1)}-02-15')\n",
    "    \n",
    "dates_sentinel_1 = (f'{str(year)}-01-01' , f'{str(year)}-12-31')\n",
    "\n",
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)\n",
    "\n",
    "#y = data['Y_tile'][13]\n",
    "#x = data['X_tile'][13] # 1607, 1166, \n",
    "\n",
    "x = 499\n",
    "y = 1377\n",
    "download_tile(x = x, y = y, data = data)\n",
    "s2, dates, interp, s1 = process_tile(x = x, y = y, data = data)\n",
    "process_subtiles(x, y, s2, dates, interp, s1)\n",
    "time2 = time()\n",
    "print(f\"Finished in {np.around(time2 - time1, 1)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "from glob import glob\n",
    "li_dirs = glob(\"../project-monitoring/tof/*/*\")\n",
    "\n",
    "\n",
    "print(li_dirs)\n",
    "li_all_files = list()\n",
    "for folder in li_dirs:\n",
    "    files = [file for file in os.listdir(folder) if os.path.splitext(file)[-1] == '.tif']\n",
    "    for file in files:\n",
    "        li_all_files.append(os.path.join(folder, file))\n",
    "\n",
    "gdal.BuildVRT('out.vrt', li_all_files)\n",
    "\n",
    "#!gdal_translate -of GTiff out.vrt out.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdalwarp -s_srs \"EPSG:4326\" -t_srs \"EPSG:32663\" -of vrt in2.tif out.vrt\n",
    "!gdal_translate -co compress=LZW out.vrt out.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
