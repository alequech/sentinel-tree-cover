{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, preprocess, and save train and test data\n",
    "# John Brandt\n",
    "# April 1, 2020\n",
    "\n",
    "- Fuse Sentinel 1/2 data\n",
    "- Reconstruct 2D-array from CEO output CSV by plot\n",
    "- Match sentinel data to CEO labels\n",
    "- Stack data_x, data_y, length\n",
    "- Save numpy arrays for data_x, data_y, length\n",
    "\n",
    "The notebook additionally contains some development code for:\n",
    "- Parameter selection in whittaker smoothing\n",
    "- Graphing plot locations on map\n",
    "\n",
    "# Package imports and source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "%run ../src/preprocessing/slope.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(plot_id):\n",
    "    '''Takes a plot ID and subsets the input pd.DataFrame to that plot ID\n",
    "       returns a (14, 14) array-like list with binary labels\n",
    "       \n",
    "        Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'train'\n",
    "sentinel_1 = True\n",
    "s2_path = \"../data/{}-s2-new/\".format(source)\n",
    "s1_path = \"../data/{}-s1-new/\".format(source)\n",
    "csv_path = \"../data/{}-csv/\".format(source)\n",
    "output_path = \"../data/{}-processed/\".format(source)\n",
    "dem_path = \"../data/{}-dem/\".format(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n",
      "992\n"
     ]
    }
   ],
   "source": [
    "# Load and edit bad plot ids if needed\n",
    "verified_lu_change = np.load(\"bad_plot_ids.npy\")\n",
    "len(verified_lu_change)\n",
    "\n",
    "#verified_lu_change = np.concatenate([verified_lu_change, \n",
    "#                     np.array([138948201, 138948267, 138948365, 138948427, 138948534]).flatten()])\n",
    "print(len(verified_lu_change))\n",
    "\n",
    "to_remove = []\n",
    "\n",
    "verified_lu_change = [x for x in verified_lu_change if x not in to_remove]\n",
    "np.save(\"bad_plot_ids.npy\", np.array(verified_lu_change))\n",
    "print(len(verified_lu_change))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanzania-region-val.csv\n",
      "ceo-brazil-finetune-sample-data-2020-09-14.csv\n",
      "ghana-kwofu-train.csv\n",
      "kenya-makueni-train.csv\n",
      "cameroon-finetune-3.csv\n",
      "ceo-makueni-fix-2-sample-data-2020-10-22.csv\n",
      "ceo-brazil-mid-coast-sample-data-2020-07-24.csv\n",
      "lac-south-train.csv\n",
      "india-sidhi-train.csv\n",
      "cameroon-finetune-2.csv\n",
      "koure-finetune.csv\n",
      "todo-brazil-north.csv\n",
      "kenya-makueni-train-2.csv\n",
      "ceo-brazil-paraiba-train-sample-data-2020-07-22.csv\n",
      "train-honduras-2.csv\n",
      "kenya-planet.csv\n",
      "ghana-ashanti-train-2.csv\n",
      "africa-east-train.csv\n",
      "mexico-campeche-train.csv\n",
      "ghana-farm-train.csv\n",
      "sa-train.csv\n",
      "europe-sw-asia-train.csv\n",
      "ghana-south-train.csv\n",
      "centralasia-train.csv\n",
      "todo-afr-south.csv\n",
      "ceo-lac-random-points-sample-data-2020-07-29.csv\n",
      "honduras-2-train.csv\n",
      "cameroonnigerghana-train.csv\n",
      "train-sa-west.csv\n",
      "ceo-br-gain-4-sample-data-2020-09-29.csv\n",
      "malawi-rumphi-train.csv\n",
      "ceo-elsalvador-train-sample-data-2020-07-22.csv\n",
      "ghana-ashanti-train-small.csv\n",
      "ghana-train.csv\n",
      "ceo-south-central-america-train.csv\n",
      "kenya-train.csv\n",
      "ceo-brazil-gain-overall-sample-data-2020-10-08.csv\n",
      "india-train.csv\n",
      "ceo-brazil-south-small-sample-data-2020-07-23.csv\n",
      "ceo-makueni-fix-sample-data-2020-10-20.csv\n",
      "ceo-brazil-finetune2-sample-data-2020-09-14.csv\n",
      "ceo-brazil-sao-paulo-sample-data-2020-07-29.csv\n",
      "malawi-rumphi-small.csv\n",
      "ceo-brazil-south-sample-data-2020-07-23.csv\n",
      "ceo-el-salvador-train-sample-data-2020-07-29.csv\n",
      "honduras-train.csv\n",
      "ceo-guatemala-new-sample-data-2020-07-23.csv\n",
      "rwanda-train.csv\n",
      "cameroon-train.csv\n",
      "hyperarid-train.csv\n",
      "subplot4.csv\n",
      "kenya-farm-train-2.csv\n",
      "honduras-region-val.csv\n",
      "india-kochi-train.csv\n",
      "subplot.csv\n",
      "southamerica-train.csv\n",
      "todo-sahel.csv\n",
      "ceo-br-gain-3-sample-data-2020-09-30.csv\n",
      "sudan-train.csv\n",
      "ceo-guatemala-train-1-sample-data.csv\n",
      "ceo-ethiopia-finetune-sample-data.csv\n",
      "ceo-brazil-sao-paulo-finetune2-sample-data-2020-09-18.csv\n",
      "ceo-brazil-random-sample-data-2020-07-29.csv\n",
      "niger-train.csv\n",
      "subplot2.csv\n",
      "ceo-brazil-gain-sample-data-2020-09-18.csv\n",
      "honduras-val.csv\n",
      "subplot3.csv\n",
      "tanzania-subregion-val.csv\n",
      "ceo-lac-north-train-sample-data-2020-07-22.csv\n",
      "northamerica-train.csv\n",
      "ghana-ashanti-train.csv\n",
      "ghana-upperwest-train.csv\n",
      "ceo-br-gain-5-sample-data-2020-09-30.csv\n",
      "malawi-train.csv\n",
      "honduras-train-north.csv\n",
      "africa-west-train.csv\n",
      "ghana-kwofu-large.csv\n",
      "lac-train.csv\n",
      "ghana-mid-train.csv\n",
      "ghana-kwofu-small-train.csv\n",
      "ghana-region-val-2.csv\n",
      "cameroon-finetune.csv\n",
      "kenya-farm-train.csv\n",
      "ghana-region-val-1.csv\n",
      "australia-train.csv\n",
      "ceo-brazil-sao-paulo-2-sample-data-2020-07-29.csv\n",
      "koure-train.csv\n",
      "train-sa-north.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6817 plots\n"
     ]
    }
   ],
   "source": [
    "# For either train or test data, loop through each plot and determine whether there is\n",
    "# labelled Y data for it -- returning one dataframe for the entire data set\n",
    "\n",
    "cols_to_keep = ['PLOT_ID', 'SAMPLE_ID', 'LON', 'LAT', 'FLAGGED', 'ANALYSES', 'USER_ID',\n",
    "       'COLLECTION_TIME', 'ANALYSIS_DURATION', 'TREE']\n",
    "csvs = [x for x in os.listdir(csv_path) if '.csv' in x]\n",
    "\n",
    "dfs = []\n",
    "for i in csvs:\n",
    "    print(i)\n",
    "    df = pd.read_csv(csv_path + i, encoding = \"ISO-8859-1\")\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column not in cols_to_keep:\n",
    "            df = df.drop(column, axis = 1)\n",
    "    df['country'] = i.split(\".\")[0]\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index = True)\n",
    "df = df[~pd.isna(df['TREE'])]\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "plot_ids_loaded = plot_ids\n",
    "\n",
    "print(f\"There are {len(plot_ids)} plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to int16, reducing storage costs by three-fold'''\n",
    "    assert np.min(array) >= 0, np.min(array)\n",
    "    assert np.max(array) <= 1, np.max(array)\n",
    "    \n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "def process_dem(dem):\n",
    "    dem =  median_filter(dem, size = 5)\n",
    "    dem = calcSlope(dem.reshape((1, 32+2, 32+2)),\n",
    "                      np.full((32+2, 32+2), 10),\n",
    "                      np.full((32+2, 32+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    dem = dem / 90\n",
    "    dem = dem.reshape((32+2, 32+2, 1))\n",
    "    dem = dem[1:-1, 1:-1]\n",
    "    dem = median_filter(dem, 5)[4:-4, 4:-4]\n",
    "    return dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02af14527644211a883a0e65057dc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6817), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 5939 plots\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7c431677e44a20871bce85187dc100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5939), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing [231]\n",
      "Finished loading: (5938, 13, 24, 24, 17) of uint16 type\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "dataframe = pd.DataFrame({'plot_id': [''], 'lat': [0.325], 'long': [0.325]})\n",
    "\n",
    "# Identify shape of data to load\n",
    "plot_ids_to_load = []\n",
    "for i in tnrange(len(plot_ids)):\n",
    "    s1_i = f'{s1_path}{str(plot_ids[i])}.npy'\n",
    "    s2_i = f'{s2_path}{str(plot_ids[i])}.npy'\n",
    "    dem_i = f'{dem_path}{str(plot_ids[i])}.npy'\n",
    "    if os.path.isfile(s2_i) and os.path.isfile(s1_i):\n",
    "        if plot_ids[i] not in verified_lu_change:\n",
    "            plot_ids_to_load.append(plot_ids[i])\n",
    "\n",
    "print(f\"There are {len(plot_ids_to_load)} plots\")\n",
    "data_x = np.zeros((len(plot_ids_to_load), 13, 24, 24, 17)).astype(np.uint16)\n",
    "data_y = np.zeros((len(plot_ids_to_load), 14, 14))\n",
    "            \n",
    "    \n",
    "\n",
    "# Iterate over each plot\n",
    "to_remove = []\n",
    "for i in tnrange(len(plot_ids_to_load)):\n",
    "    s1_i = f'{s1_path}{str(plot_ids_to_load[i])}.npy'\n",
    "    s2_i = f'{s2_path}{str(plot_ids_to_load[i])}.npy'\n",
    "    dem_i = f'{dem_path}{str(plot_ids_to_load[i])}.npy'\n",
    "\n",
    "    x = np.load(s2_i)\n",
    "    s1 = np.load(s1_i)\n",
    "    s1_median = np.median(s1, axis = 0)\n",
    "    s1 = np.concatenate([s1, s1_median[np.newaxis]], axis = 0)\n",
    "    dem = np.load(dem_i)\n",
    "    dem = process_dem(dem)\n",
    "    dem = np.tile(dem.reshape((1, 24, 24)), (x.shape[0], 1, 1))\n",
    "    x[..., 10] = dem\n",
    "    x = np.concatenate([x, s1], axis = -1)\n",
    "    count += 1\n",
    "    y = reconstruct_images(plot_ids_to_load[i])\n",
    "    long = np.mean(df[df['PLOT_ID'] == plot_ids_to_load[i]]['LON'])\n",
    "    lat = np.mean(df[df['PLOT_ID'] == plot_ids_to_load[i]]['LAT'])\n",
    "    dataframe = dataframe.append({'plot_id': str(plot_ids_to_load[i]), 'lat': lat, 'long': long}, ignore_index = True)\n",
    "    dataframe.append([plot_ids_to_load[i], lat, long])\n",
    "    # The indices can range from -1 to 1, clip to 0-1\n",
    "    x[..., 11:15] = np.clip(x[..., 11:15], -1, 1)\n",
    "    x[..., 11:15] = (x[..., 11:15] + 1) / 2\n",
    "    if np.sum(np.isnan(x)) > 0:\n",
    "        to_remove.append(i)\n",
    "    else:\n",
    "        x = np.clip(x, 0, 1)\n",
    "        x = to_int16(x)\n",
    "        data_x[i] = x\n",
    "        data_y[i] = y\n",
    "            \n",
    "# Remove any data samples that had missing values\n",
    "if len(to_remove) > 0:\n",
    "    print(f\"Removing {to_remove}\")\n",
    "    data_x = np.delete(data_x, to_remove, 0)\n",
    "    data_y = np.delete(data_y, to_remove, 0)\n",
    "            \n",
    "print(f\"Finished loading: {data_x.shape} of {data_x.dtype} type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train data\n"
     ]
    }
   ],
   "source": [
    "import hickle as hkl\n",
    "dataframe = dataframe.drop(0, 0)\n",
    "dataframe.reset_index(inplace = True, drop = True)\n",
    "\n",
    "print(f\"Writing {source} data\")\n",
    "hkl.dump(data_x, f\"../tile_data/{source}/{source}_x.hkl\", mode='w', compression='gzip')\n",
    "hkl.dump(data_y, f\"../tile_data/{source}/{source}_y.hkl\", mode='w', compression='gzip')\n",
    "dataframe.to_csv(f\"../tile_data/{source}/{source}_plot_ids.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
